transformers ~= 4.47.1
accelerate >= 0.12.0
torch ==2.7.0
datasets >= 2.14.0
sentencepiece != 0.1.92
protobuf
evaluate
scikit-learn
liger-kernel

packaging
# https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl
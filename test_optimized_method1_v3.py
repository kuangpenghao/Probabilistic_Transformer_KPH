#!/usr/bin/env python3
import sys
import os
import torch
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_optimized_method1_v3():
    print("æµ‹è¯•ä¼˜åŒ–åçš„ Method1_v3...")
    
    try:
        from models.version3.configuration_llama_v3 import Method1Config_v3
        from models.version3.Method1_v3 import Method1LlamaForCausalLM_v3
        print("âœ“ æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºä¸€ä¸ªå°å‹é…ç½®ç”¨äºæµ‹è¯•
        config = Method1Config_v3(
            hidden_size=64,
            intermediate_size=128,
            num_hidden_layers=3,
            num_attention_heads=4,
            num_key_value_heads=4,
            vocab_size=1000,
            max_position_embeddings=512,
            rms_norm_eps=1e-6,
        )
        print("âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ¨¡å‹
        model = Method1LlamaForCausalLM_v3(config)
        model.eval()
        print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        seq_length = 8
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
        attention_mask = torch.ones(batch_size, seq_length)
        
        print("âœ“ æµ‹è¯•è¾“å…¥åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_attentions=False,
                output_hidden_states=False
            )
        
        print("âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"è¾“å‡ºlogits shape: {outputs.logits.shape}")
        
        # æµ‹è¯•è‡ªå®šä¹‰attentionæ–¹æ³•
        attention = model.model.layers[0].self_attn
        hidden_states = torch.randn(batch_size, seq_length, config.hidden_size)
        v_weight = attention.v_proj.weight
        
        # åˆ›å»ºå‡çš„Qã€KçŸ©é˜µç”¨äºæµ‹è¯•
        query_states = torch.randn(batch_size, config.num_attention_heads, seq_length, config.hidden_size // config.num_attention_heads)
        key_states = torch.randn(batch_size, config.num_key_value_heads, seq_length, config.hidden_size // config.num_key_value_heads)
        
        # æµ‹è¯•forward_with_precomputed_qkvæ–¹æ³•
        attn_output = attention.forward_with_precomputed_qkv(
            hidden_states=hidden_states,
            precomputed_query_states=query_states,
            precomputed_key_states=key_states,
            v_proj_weight=v_weight,
            attention_mask=None,
            position_embeddings=None,
        )
        print(f"âœ“ forward_with_precomputed_qkv æˆåŠŸ, è¾“å‡ºshape: {attn_output.shape}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¼˜åŒ–åçš„Method1_v3å·¥ä½œæ­£å¸¸ï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_optimized_method1_v3()
    if success:
        print("\nâœ… ä¼˜åŒ–åçš„Method1_v3æµ‹è¯•å®Œæˆï¼ŒåŠŸèƒ½æ­£å¸¸ï¼")
    else:
        print("\nâŒ ä¼˜åŒ–å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")

import math
import warnings
from typing import List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch import nn
from torch.nn import CrossEntropyLoss

from transformers.cache_utils import Cache
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaModel, LlamaDecoderLayer, LlamaAttention
from .configuration_llama_v3 import LlamaConfig, Method2Config_v3


class ModifiedResidualAttention:
    """
    å°è£…ä¿®æ”¹åŽçš„Attentionæ®‹å·®è¿žæŽ¥é€»è¾‘ï¼Œä¾¿äºŽåœ¨ä¸åŒæ–¹æ³•ä¸­å¤ç”¨å’Œç»§æ‰¿
    """
    def __init__(self, layer_idx: int):
        self.layer_idx = layer_idx
    
    def compute_residual(self, previous_attn_outputs: Optional[List[torch.Tensor]], 
                        residual: torch.Tensor, attn_output: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¿®æ”¹åŽçš„Attentionæ®‹å·®è¿žæŽ¥
        
        Args:
            previous_attn_outputs: å‰é¢å±‚çš„Attentionè¾“å‡ºåˆ—è¡¨
            residual: å½“å‰å±‚çš„åŽŸå§‹è¾“å…¥ï¼ˆattentionå‰çš„è¾“å…¥ï¼‰
            attn_output: å½“å‰å±‚Attentionçš„è¾“å‡º
            
        Returns:
            æœ€ç»ˆçš„attentionåŽè¾“å‡º
        """
        if self.layer_idx == 0:
            # ç¬¬ä¸€å±‚ï¼šä½¿ç”¨æ ‡å‡†æ®‹å·®è¿žæŽ¥
            return residual + attn_output
        else:
            # å…¶ä»–å±‚ï¼šä½¿ç”¨é‡æ–°è®¡ç®—çš„Attentionè¾“å‡ºä½œä¸ºæ®‹å·®
            if previous_attn_outputs is not None and len(previous_attn_outputs) > 0:
                residual_sum = sum(previous_attn_outputs)
                return residual_sum + attn_output
            else:
                # å¦‚æžœæ²¡æœ‰æä¾›ä¹‹å‰çš„è¾“å‡ºï¼Œå›žé€€åˆ°åŽŸå§‹è¡Œä¸º
                return residual + attn_output


class Method2LlamaAttention_v3(LlamaAttention):
    """
    Method2ç‰ˆæœ¬çš„è‡ªå®šä¹‰Attentionç±»ï¼Œå­˜å‚¨attn_weightsã€Væƒé‡ã€Oæƒé‡æ¥é‡æ–°è®¡ç®—attention
    ç”¨äºŽAttentionå¤„æ®‹å·®è¿žæŽ¥ä¿®æ”¹
    """
    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__(config, layer_idx)
        
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = True,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache], torch.Tensor, torch.Tensor]:
        # è°ƒç”¨çˆ¶ç±»çš„forwardæ–¹æ³•èŽ·å–attentionè¾“å‡ºå’Œæƒé‡
        attention_result = super().forward( 
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=True,  # å¼ºåˆ¶èŽ·å–attn_weights
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        
        # è§£æžè¿”å›žå€¼
        if isinstance(attention_result, tuple):
            attn_output = attention_result[0]
            attn_weights = attention_result[1]  # è¿™æ˜¯å·²ç»è®¡ç®—å¥½çš„æ³¨æ„åŠ›æƒé‡
            past_key_value = attention_result[2] if len(attention_result) > 2 else None
        else:
            attn_output = attention_result
            attn_weights = None
            past_key_value = None
        
        # è¿”å›žattentionè¾“å‡ºã€åŽŸå§‹æƒé‡ã€past_key_valueã€æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€Væƒé‡ã€Oæƒé‡ï¼ˆä¸å«biasï¼‰
        return (attn_output, attn_weights if output_attentions else None, past_key_value, 
                attn_weights, self.v_proj.weight, self.o_proj.weight)
    
    def forward_with_precomputed_weights(
        self,
        hidden_states: torch.Tensor,
        attn_weights: torch.Tensor,
        v_proj_weight: torch.Tensor,
        o_proj_weight: torch.Tensor,  # è¾“å‡ºæŠ•å½±æƒé‡
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        apply_strict_causal_mask: bool = True,
        **kwargs,
    ) -> torch.Tensor:
        """
        ä½¿ç”¨é¢„è®¡ç®—çš„æ³¨æ„åŠ›æƒé‡ã€Væƒé‡ã€Oæƒé‡è¿›è¡Œattentionè®¡ç®—
        çŽ°åœ¨åŒ…å«ä¸¥æ ¼çš„causal maské‡æ–°åº”ç”¨ä»¥é˜²æ­¢ä¿¡æ¯æ³„æ¼
        
        Args:
            hidden_states: è¾“å…¥çš„éšè—çŠ¶æ€ï¼ˆå½“å‰å±‚çš„è¾“å…¥ï¼‰
            attn_weights: é¢„è®¡ç®—çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µ (bsz, num_heads, q_len, k_len)
            v_proj_weight: é¢„è®¡ç®—çš„VæŠ•å½±æƒé‡
            o_proj_weight: é¢„è®¡ç®—çš„OæŠ•å½±æƒé‡ï¼ˆè¾“å‡ºæŠ•å½±ï¼‰
            attention_mask: æ³¨æ„åŠ›æŽ©ç ï¼Œç”¨äºŽé‡æ–°åº”ç”¨causalçº¦æŸ
            position_ids: ä½ç½®IDï¼ˆä¿æŒæŽ¥å£ä¸€è‡´æ€§ï¼‰
            cache_position: ç¼“å­˜ä½ç½®ï¼ˆä¿æŒæŽ¥å£ä¸€è‡´æ€§ï¼‰
            apply_strict_causal_mask: æ˜¯å¦å¯ç”¨ä¸¥æ ¼çš„causal maské‡æ–°åº”ç”¨
            
        Returns:
            attentionè¾“å‡º
        """
        bsz, q_len, _ = hidden_states.size()
        
        # é‡æ–°è®¡ç®—VçŸ©é˜µ,Reshape Vä¸ºattentionæ‰€éœ€æ ¼å¼,é‡å¤Vä»¥åŒ¹é…æ³¨æ„åŠ›å¤´æ•°
        value_states = F.linear(hidden_states, v_proj_weight, bias=self.v_proj.bias)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = self._repeat_kv(value_states, self.num_key_value_groups)
        
        # ä½¿ç”¨å¤„ç†åŽçš„æ³¨æ„åŠ›æƒé‡ä¸Žæ–°çš„Vç›¸ä¹˜
        attn_output = torch.matmul(attn_weights, value_states)
        
        # Reshapeå¹¶åº”ç”¨é¢„è®¡ç®—çš„è¾“å‡ºæŠ•å½±ï¼ˆæƒé‡ï¼Œä¸å«åç½®ï¼‰
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        attn_output = F.linear(attn_output, o_proj_weight, bias=self.o_proj.bias)
        
        return attn_output
    
    def _repeat_kv(self, hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
        batch, num_key_value_heads, slen, head_dim = hidden_states.shape
        if n_rep == 1:
            return hidden_states
        hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)

class Method2DecoderLayer_v3(LlamaDecoderLayer):
    def __init__(self, config, layer_idx: int):
        super().__init__(config, layer_idx)
        self.layer_idx = layer_idx
        # ä½¿ç”¨è‡ªå®šä¹‰çš„Attentionç±»
        self.self_attn = Method2LlamaAttention_v3(config=config, layer_idx=layer_idx)
        # åˆå§‹åŒ–ä¿®æ”¹åŽçš„Attentionæ®‹å·®è¿žæŽ¥å¤„ç†å™¨
        self.modified_residual_attn = ModifiedResidualAttention(layer_idx)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        # æ–°å¢žå‚æ•°ï¼šå­˜å‚¨ä¹‹å‰å±‚çš„Attentionè¾“å‡ºå’Œæƒé‡çŸ©é˜µ
        previous_attn_outputs: Optional[List[torch.Tensor]] = None,
        stored_weights: Optional[dict] = None,  # å­˜å‚¨attn_weightsã€Væƒé‡ã€Oæƒé‡
        current_layer_input: Optional[torch.Tensor] = None,  # å½“å‰å±‚çš„åŽŸå§‹è¾“å…¥
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]], Optional[torch.Tensor], dict]:
        
        # ä¿å­˜å½“å‰å±‚è¾“å…¥
        residual = hidden_states
        
        # è¾“å…¥å±‚å½’ä¸€åŒ–
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention - ä½¿ç”¨è‡ªå®šä¹‰çš„attentionç±»
        attn_result = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        
        # å¤„ç†æ³¨æ„åŠ›æ¨¡å—çš„è¿”å›žå€¼ï¼ˆçŽ°åœ¨åŒ…å«attn_weightsã€Væƒé‡ã€Oæƒé‡ï¼‰
        if len(attn_result) >= 6:
            attn_output = attn_result[0]
            self_attn_weights = attn_result[1] if output_attentions else None
            present_key_value = attn_result[2] if use_cache else None
            stored_attn_weights = attn_result[3]  # ç”¨äºŽå­˜å‚¨çš„æ³¨æ„åŠ›æƒé‡
            v_proj_weight = attn_result[4]  # VæŠ•å½±æƒé‡
            o_proj_weight = attn_result[5]  # OæŠ•å½±æƒé‡
        else:
            # å›žé€€åˆ°åŽŸå§‹è¡Œä¸º
            attn_output = attn_result[0] if isinstance(attn_result, tuple) else attn_result
            self_attn_weights = attn_result[1] if isinstance(attn_result, tuple) and len(attn_result) > 1 and output_attentions else None
            present_key_value = attn_result[2] if isinstance(attn_result, tuple) and len(attn_result) > 2 and use_cache else None
            stored_attn_weights = None
            v_proj_weight = None
            o_proj_weight = None
        
        # å­˜å‚¨å½“å‰å±‚çš„æƒé‡ä¿¡æ¯
        current_weights = {
            'attn_weights': stored_attn_weights,  # æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
            'v_proj_weight': v_proj_weight,       # VæŠ•å½±æƒé‡
            'o_proj_weight': o_proj_weight,       # OæŠ•å½±æƒé‡
            'input_layernorm': self.input_layernorm  # è¾“å…¥å±‚å½’ä¸€åŒ–
        }
        
        # ä½¿ç”¨ModifiedResidualAttentionå¤„ç†æ®‹å·®è¿žæŽ¥
        hidden_states = self.modified_residual_attn.compute_residual(
            previous_attn_outputs, residual, attn_output
        )

        # MLPéƒ¨åˆ†ä¿æŒåŽŸå§‹çš„æ®‹å·®è¿žæŽ¥
        mlp_residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_output = self.mlp(hidden_states)
        hidden_states = mlp_residual + mlp_output

        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        if use_cache:
            outputs += (present_key_value,) 
        
        # æ·»åŠ å½“å‰å±‚çš„æƒé‡ä¿¡æ¯åˆ°è¿”å›žå€¼ä¸­
        outputs += (current_weights,)

        return outputs


class Method2LlamaModel_v3(LlamaModel):
    config_class = Method2Config_v3

    def __init__(self, config: Method2Config_v3):
        super().__init__(config)
        # æ›¿æ¢æ‰€æœ‰çš„decoder layerä¸ºæ–°çš„å®žçŽ°
        self.layers = nn.ModuleList(
            [Method2DecoderLayer_v3(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        
        # é‡æ–°åˆå§‹åŒ–æƒé‡
        self.post_init()

    def _recompute_previous_attn_outputs(self, current_input: torch.Tensor, stored_weights: List[dict], 
                                       layer_idx: int, position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]],
                                       attention_mask: Optional[torch.Tensor] = None,
                                       position_ids: Optional[torch.LongTensor] = None,
                                       cache_position: Optional[torch.LongTensor] = None) -> List[torch.Tensor]:
        """
        é‡æ–°è®¡ç®—å‰é¢æ‰€æœ‰å±‚çš„Attentionè¾“å‡ºï¼Œä½¿ç”¨å½“å‰å±‚çš„è¾“å…¥åµŒå…¥
        ä½¿ç”¨å­˜å‚¨çš„attn_weightsã€Væƒé‡ã€Oæƒé‡ï¼Œå¤§å¹…ç®€åŒ–è®¡ç®—å¤æ‚åº¦
        çŽ°åœ¨åŒ…å«ä¸¥æ ¼çš„causal maskéªŒè¯ä»¥é˜²æ­¢ä¿¡æ¯æ³„æ¼
        
        Args:
            current_input: å½“å‰å±‚çš„è¾“å…¥åµŒå…¥
            stored_weights: å­˜å‚¨çš„æ‰€æœ‰å‰é¢å±‚çš„æƒé‡ä¿¡æ¯ï¼ˆåŒ…å«attn_weights, v_proj_weight, o_proj_weightï¼‰
            layer_idx: å½“å‰å±‚ç´¢å¼•
            position_embeddings: ä½ç½®åµŒå…¥ï¼ˆæœªä½¿ç”¨ï¼Œä¿æŒæŽ¥å£ä¸€è‡´æ€§ï¼‰
            attention_mask: æ³¨æ„åŠ›æŽ©ç ï¼ˆçŽ°åœ¨çœŸæ­£ä½¿ç”¨ï¼Œç”¨äºŽcausalçº¦æŸï¼‰
            position_ids: ä½ç½®IDï¼ˆä¼ é€’ç»™attentionæ–¹æ³•ï¼‰
            cache_position: ç¼“å­˜ä½ç½®ï¼ˆä¼ é€’ç»™attentionæ–¹æ³•ï¼‰
            
        Returns:
            é‡æ–°è®¡ç®—çš„å‰é¢æ‰€æœ‰å±‚çš„Attentionè¾“å‡ºåˆ—è¡¨
        """
        recomputed_attn_outputs = []
        
        for i in range(layer_idx):
            weights = stored_weights[i]
            layer = self.layers[i]
            
            # èŽ·å–å­˜å‚¨çš„æ³¨æ„åŠ›æƒé‡ã€Væƒé‡ã€Oæƒé‡
            attn_weights = weights['attn_weights']
            v_proj_weight = weights['v_proj_weight']
            o_proj_weight = weights['o_proj_weight']
            
            if v_proj_weight is None or attn_weights is None or o_proj_weight is None:
                continue
                
            # å¯¹å½“å‰è¾“å…¥è¿›è¡ŒLayerNormï¼ˆç¬¬iå±‚çš„input_layernormï¼‰
            input_layernorm = weights['input_layernorm']
            normalized_input = input_layernorm(current_input)
            
            # ðŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸¥æ ¼çš„causal maskçº¦æŸ
            attn_output = layer.self_attn.forward_with_precomputed_weights(
                hidden_states=normalized_input,
                attn_weights=attn_weights,
                v_proj_weight=v_proj_weight,
                o_proj_weight=o_proj_weight,  # ä¼ é€’Oæƒé‡
                attention_mask=attention_mask,  # çŽ°åœ¨ä¼ é€’mask
                position_ids=position_ids,
                cache_position=cache_position,
                apply_strict_causal_mask=True,  # å¯ç”¨ä¸¥æ ¼causalçº¦æŸ
            )
            
            # ç›´æŽ¥è¾“å‡ºattentionç»“æžœï¼Œä¸è¿›è¡Œæ®‹å·®è¿žæŽ¥
            recomputed_attn_outputs.append(attn_output)
            
        return recomputed_attn_outputs

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if input_ids is None and inputs_embeds is None:
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            warnings.warn(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        if not isinstance(past_key_values, (type(None), Cache)):
            raise ValueError("The `past_key_values` should be either a `Cache` object or `None`.")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            from transformers.cache_utils import DynamicCache
            past_key_values = DynamicCache()

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        causal_mask = self._update_causal_mask(
            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
        )

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # å­˜å‚¨æ‰€æœ‰å±‚çš„æƒé‡ä¿¡æ¯å’Œå±‚è¾“å…¥
        stored_weights = []
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None

        for layer_idx, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
            
            # é‡æ–°è®¡ç®—å‰é¢å±‚çš„Attentionè¾“å‡ºï¼ˆä½¿ç”¨å½“å‰å±‚è¾“å…¥ï¼‰
            if layer_idx > 0:
                recomputed_attn_outputs = self._recompute_previous_attn_outputs(
                    hidden_states, stored_weights, layer_idx, position_embeddings, 
                    causal_mask, position_ids, cache_position
                )
            else:
                recomputed_attn_outputs = []

            if self.gradient_checkpointing and self.training:
                from functools import partial
                layer_outputs = self._gradient_checkpointing_func(
                    partial(decoder_layer.__call__, **kwargs),
                    hidden_states,
                    causal_mask,
                    position_ids,
                    past_key_values,
                    output_attentions,
                    use_cache,
                    cache_position,
                    position_embeddings,
                    recomputed_attn_outputs,
                    None,  # stored_weightså‚æ•°
                    hidden_states,  # current_layer_inputå‚æ•°
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=causal_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_values,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    previous_attn_outputs=recomputed_attn_outputs,
                    stored_weights=None,
                    current_layer_input=hidden_states,
                    **kwargs,
                )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)
            
            # å­˜å‚¨å½“å‰å±‚çš„æƒé‡ä¿¡æ¯
            current_weights = layer_outputs[-1]  # æœ€åŽä¸€ä¸ªæ˜¯æƒé‡ä¿¡æ¯
            stored_weights.append(current_weights)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )


class Method2LlamaForCausalLM_v3(LlamaForCausalLM):
    config_class = Method2Config_v3

    def __init__(self, config: Method2Config_v3):
        super(LlamaForCausalLM, self).__init__(config)
        self.model = Method2LlamaModel_v3(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs,
    ) -> CausalLMOutputWithPast:
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

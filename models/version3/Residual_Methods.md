# Method1
Attentionå¤„æ®‹å·®è¿æ¥ä¸åŸå§‹æ¨¡å‹ç›¸åŒã€‚ä¿®æ”¹MLPå¤„çš„æ®‹å·®ä¸ºå…ˆå‰å±‚é‡ç®—çš„MLPè¾“å‡ºç´¯åŠ ã€‚å…ˆå‰å±‚MLPè¾“å‡ºçš„é‡ç®—æ–¹æ³•ä¸ºï¼š
* ä¿ç•™ç¬¬ä¸€æ¬¡Attentionè®¡ç®—çš„attn_weightsï¼Œ$W^V$.weight$ï¼Œ$W^O$.weightï¼Œä»…æ›´æ¢è¾“å…¥åµŒå…¥çŸ©é˜µX
* è¾“å…¥åµŒå…¥Xåšinput_norm
* Attentioné‡ç®—
* Attentionæ®‹å·®è¿æ¥
* post_attn_layernorm
* MLPè®¡ç®—
* MLPè®¡ç®—ç»“æœä¸åšæ®‹å·®è¿æ¥ç›´æ¥è¾“å‡ºï¼Œä½œä¸ºé‡ç®—åçš„MLPè¾“å‡º

# Method2
MLPå¤„æ®‹å·®è¿æ¥ä¸åŸå§‹æ¨¡å‹ç›¸åŒã€‚ä¿®æ”¹Attentionå¤„çš„æ®‹å·®ä¸ºå…ˆå‰å±‚é‡ç®—çš„Attentionè¾“å‡ºç´¯åŠ ã€‚å…ˆå‰å±‚Attentionè¾“å‡ºçš„é‡ç®—æ–¹æ³•ä¸ºï¼š
* ä¿ç•™ç¬¬ä¸€æ¬¡Attentionè®¡ç®—çš„attn_weightsï¼Œ$W^V$.weightï¼Œ$W^O$.weightï¼Œä»…æ›´æ¢è¾“å…¥åµŒå…¥çŸ©é˜µX
* è¾“å…¥åµŒå…¥Xåšinput_norm
* Attentioné‡ç®—
* Attentioné‡ç®—ç»“æœä¸åšæ®‹å·®è¿æ¥ç›´æ¥è¾“å‡ºï¼Œä½œä¸ºé‡ç®—åçš„Attentionè¾“å‡º

å³ï¼šMethod1_v3ä¸Method2_v3çš„å·®åˆ«ä¸ºï¼šæ®‹å·®è¿æ¥çš„ä¿®æ”¹ä½ç‚¹ä¸åŒï¼Œå…ˆå‰å±‚è¾“å‡ºé‡ç®—çš„æˆªæ­¢ä½ç½®ä¸åŒï¼ˆæˆªè‡³MLPè¾“å‡º/æˆªè‡³Attentionè¾“å‡ºï¼‰

# Method3
ä¸Method1åŸºæœ¬ç›¸åŒï¼Œå”¯ä¸€ä¸åŒä¹‹å¤„åœ¨äºMLPå¤„æ®‹å·®å’Œè¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œä¸”æ¯ä¸€å±‚çš„æƒé‡åˆ†å¸ƒä¸º1/m(Method 3.1)æˆ–å¯å­¦ä¹ æƒé‡(Method3.2)

# Method4
ä¸Method2åŸºæœ¬ç›¸åŒï¼Œå”¯ä¸€ä¸åŒä¹‹å¤„åœ¨äºAttentionå¤„æ®‹å·®å’Œè¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œä¸”æ¯ä¸€å±‚çš„æƒé‡åˆ†å¸ƒä¸º1/m(Method 4.1)æˆ–å¯å­¦ä¹ æƒé‡(Method4.2)

# Method5ï¼ˆæ³¨ï¼šç°ä¸ºVersion4çš„ä¸ƒç§æ–¹æ³•ï¼‰
ç›¸å½“äºPTæ‰©å……å¤šç»„Z nodeã€‚ç›´æ¥å°†æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—ç”±$Softmax(\dfrac{QK^T}{\sqrt{d_k}})$æ”¹æˆ$Softmax(\dfrac{\sum QK^T}{d_s})$ï¼ˆ$d_s$å¾…å®šï¼‰ï¼Œå³åªéœ€è¦è®°å½•ä¹‹å‰å±‚çš„$QK^T$çŸ©é˜µå†ç›´æ¥ç›¸åŠ 

* Method5.1:$d_s=\sqrt{d_k}$
* Method5.2:$d_s=\sqrt{d_k}Â·\sqrt{m}$(mä¸ºå±‚æ•°)
* Method5.3:$d_s=\sqrt{d_k}Â·m$
* Method5.4:$d_s=d_k^aÂ·m^b$(a,bä¸ºå¯å­¦ä¹ å‚æ•°)
* Method5.5:$d_s$ä¸ºå¯å­¦ä¹ çš„$\sqrt{d_k}*\vec{a}$å‘é‡ï¼ŒåŸå…ˆæ±‚å’Œé™¤ä»¥åˆ†æ¯æ“ä½œå˜ä¸º$QK^T$å‘é‡ä¸$d_s$å‘é‡ç‚¹ä¹˜

---



# Ideaæ¨å¯¼

ç”±è®ºæ–‡ä¸­å…¬å¼(27)( $G^{(t-1)}=2\sum_c{Q_{h,c}^{(t-1)}V_cU^{(c)T}}$ )å¯çŸ¥ï¼šå¤šç»„H nodeå¯¹Z nodeæ›´æ–°çš„æ¶ˆæ¯é‡ä¸º$G=2\sum_n \sum_c {Q_{h,c}^{(t-1)}V_{n.c}U^{(c)T}}$ã€‚å…¶ä¸­nä¸ºH nodeçš„ç»„åˆ«ï¼Œ$V_c=Q_z^{(t-1)}V^{(c)}$

å…¶ä¸­ï¼š$V_c$çš„ä¿¡æ¯æ¥æºäºä¸Šä¸€è½®çš„$Q_z$è€Œéæ›´æ–°è¯¥ç»„æ—¶çš„è½®æ¬¡ï¼ˆç±»æ¯”Transformerä¸­çš„ç¬¬iå±‚ï¼‰ã€‚æ‰€ä»¥ç›¸å½“äºTransformersä¸­æ³¨æ„åŠ›æ¨¡å—çš„$V$éœ€è¦é‡ç®—ã€‚

ç”±$V_c=Q_z^{(t-1)}V^{(c)}$å¯çŸ¥ï¼š$V^{(c)}$æ²¡æœ‰å‘ç”Ÿæ”¹å˜ï¼Œç›¸å½“äºTransformerå±‚ä¸­çš„$W^V$æ²¡æœ‰å‘ç”Ÿæ”¹å˜ï¼Œä»…æœ‰è¾“å…¥åµŒå…¥$X$å‘ç”Ÿæ”¹å˜ï¼Œå³PTä¸­çš„$Q_Z$è½®æ¬¡ä¸å†æ˜¯å½“æ—¶çš„è½®æ¬¡ã€‚æ‰€ä»¥é‡ç®—$V$çš„æ–¹æ³•ä¸ºï¼šæ¯å±‚éƒ½é‡ç®—ï¼Œè¾“å…¥åµŒå…¥$X$æ”¹æˆå½“å‰Transformerå±‚çš„$X$ã€‚


# å·¥ç¨‹å®ç°

### LlamaForCausalLM->Method1LlamaForCausalLM_v3
å”¯ä¸€çš„æ”¹åŠ¨åœ¨äºself.modelå˜ä¸ºè‡ªå®šä¹‰model

### LlamaModel->Method1LlamaModel_v3

**ä¸€ã€æ·»åŠ def _recompute_previous_mlp_outputsï¼Œç”¨äºé‡ç®—ä¹‹å‰å±‚çš„MLPè¾“å‡ºï¼š**
* æ¥å—å­˜å–çš„å½“å‰å±‚è¾“å…¥åµŒå…¥Xã€ä¹‹å‰å±‚å­˜å–çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µ($Softmax(\dfrac{QK^T}{\sqrt{d_k}})$)ã€$W^V$ã€MLPä¸normalizationè®¡ç®—æ¨¡å—ç­‰ä¿¡æ¯
* é€å±‚å¾ªç¯
  * è°ƒç”¨forward_with_precomputed_weightså‡½æ•°ï¼ˆå®šä¹‰åœ¨class Method1LlamaAttention_v3ä¸­ï¼‰ï¼Œç›´æ¥åˆ©ç”¨å­˜å–çš„attn_weightsã€$W^V$ç­‰ä¸è¾“å…¥åµŒå…¥Xè¿›è¡Œæ³¨æ„åŠ›è¿ç®—ï¼Œå¹¶åŠ ä¸Šcausal maskï¼ˆ301 304è¡Œï¼‰
  * ç›´æ¥åˆ©ç”¨å­˜å–çš„mlpæ¨¡å—åšMLPè¿ç®—
  * å­˜å‚¨è¯¥å±‚é‡ç®—åçš„MLPè¾“å‡º
* è¿”å›é‡ç®—åçš„MLPè¾“å‡ºåˆ—è¡¨recomputed_mlp_outputs

**äºŒã€forwardå‡½æ•°ä¿®æ”¹**
* ç”¨stored_weights(379è¡Œ)å­˜å–å…ˆå‰å±‚çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µå’Œ$W^V$ï¼Œç”¨äºå‘_recompute_previous_mlp_outputsä¼ å‚
* _recompute_previous_mlp_outputsè®¡ç®—çš„recomputed_mlp_outputsï¼Œå‘DecoderLayerä¼ å‚ï¼Œä½œä¸ºå…ˆå‰å±‚çš„æ®‹å·®ï¼Œæ›´æ–°MLPçš„æ®‹å·®è¿æ¥

### LlamaDecoderLayer->Method1DecoderLayer_v3

**ä¸€ã€attentionæ¨¡å—ä¿®æ”¹**
* self.self_attnæ”¹ä¸ºè‡ªå®šä¹‰çš„attentionå®ç°æ¨¡å—Method1LlamaAttention_v3

**äºŒã€forwardå‡½æ•°ä¿®æ”¹**
* è°ƒç”¨æ–°çš„è‡ªå®šä¹‰self_attnæ¨¡å—
* self_attnæœŸæœ›è¿”å›attn_outputï¼Œattn_weightsï¼ˆ$Softmax(\dfrac{QK^T}{\sqrt{d_k}})$ï¼‰ï¼Œpresent_key_valueï¼Œstored_attn_weightsï¼ˆä¸attn_weightså®Œå…¨ç›¸åŒï¼Œæ­¤å¤„å†™é‡å¤äº†ï¼‰ï¼Œv_proj_weightï¼ˆ$W^V$ï¼‰ï¼Œå°†ç¼“å­˜çš„attn_weightsã€$W^V$ä¸self.mlp\å­˜å‚¨è‡³å­—å…¸current_weightså½“ä¸­
* Attentionéƒ¨åˆ†æ®‹å·®è¿æ¥éµå¾ªåŸæ¨¡å‹ï¼ŒMLPéƒ¨åˆ†æ®‹å·®æ”¹ä¸ºé‡ç®—è¿‡çš„previous_mlp_outputsï¼ˆç”±ä¿®æ”¹åçš„LlamaModelä¼ å‚è€Œæ¥ï¼‰æ±‚å’Œ
* è¿”å›å€¼ä¸ºï¼šDecoderlayeræœ€ç»ˆè¾“å‡º+self_attn_weights+present_key_value+å­—å…¸current_weightsã€‚ç›¸æ¯”åŸå§‹çš„DecoderLayerè¾“å‡ºå¤šäº†ä¸€ä¸ªå­—å…¸current_weights

### LlamaAttention->Method1LlamaAttention_v3

**ä¸€ã€forwardå‡½æ•°ä¿®æ”¹**
* é¦–å…ˆè°ƒç”¨çˆ¶ç±»forwardæ–¹æ³•ï¼Œå¾—åˆ°attn_output, attn_weights, past_key_valueåˆå¹¶ä¸ºattention_result
* è¿”å›ï¼šattn_output, attn_weights, past_key_value,attn_weights,self.v_proj.weightã€‚ç›¸æ¯”åŸå§‹çš„attentionè¿”å›å€¼ï¼Œå¤šäº†attn_weights,self.v_proj.weightï¼ˆ$Softmax(\dfrac{QK^T}{\sqrt{d_k}})$å’Œ$W^V$ï¼‰

**äºŒã€forward_with_precomputed_weightså‡½æ•°**
* åˆ©ç”¨æ–°çš„è¾“å…¥åµŒå…¥Xï¼Œé‡ç®—VçŸ©é˜µ
* å¯¹attn_weightsï¼ˆ$Softmax(\dfrac{QK^T}{\sqrt{d_k}})$ï¼‰åº”ç”¨causal mask
* é‡ç®—æ³¨æ„åŠ›è¾“å‡º

# Problems

ç›®å‰è®­ç»ƒä»å­˜åœ¨è¿‡æ‹Ÿåˆæƒ…å†µï¼ˆlossè¶‹è¿‘äº0ï¼Œaccuracyè¶‹è¿‘äº1ï¼‰

### Causal Maskä¼ é€’è·¯å¾„åˆ†æ

* åœ¨Method1LlamaNodel_v3ä¸­ç”±self._update_causal_maskç”Ÿæˆï¼ˆ369è¡Œï¼‰
  * æ¯å±‚å¾ªç¯æ—¶ï¼Œå°†causal maskä¼ å‚è‡³_recompute_previous_mlp_outputsï¼Œå‚ä¸å…ˆå‰å±‚mlpè¾“å‡ºçš„é‡ç®—
    * _recompute_previous_mlp_outputsä¸­ï¼Œä¼ å‚è‡³attentionæ¨¡å—çš„forward_with_precomputed_weightsï¼Œå‚ä¸æ³¨æ„åŠ›æƒé‡çš„é‡ç®—
  * æ¯å±‚å¾ªç¯æ—¶ï¼ŒåŒæ ·ä¼ å‚è‡³decoder_layerï¼Œå‚ä¸å½“å‰å±‚è¾“å‡ºçš„è®¡ç®—
    * ä¼ å‚è‡³self.self_attnæ¨¡å—ï¼Œå‚ä¸å½“å‰å±‚æ³¨æ„åŠ›è¾“å‡ºçš„è®¡ç®—

æ€»ç»“ï¼šattention maskåˆ†ä¸¤è·¯ä¼ é€’ï¼Œåˆ†åˆ«ä¼ é€’è‡³å½“å‰å±‚æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å’Œå…ˆå‰æ‰€æœ‰å±‚mlpè¾“å‡ºé‡ç®—ä¸­çš„æ³¨æ„åŠ›é‡ç®—æ¨¡å—

```mermaid
graph TD
    %% è¾“å…¥å’Œåˆå§‹åŒ–
    A[Method1LlamaModel_v3.forward] --> B[è¾“å…¥å¤„ç†å’Œcausal_maskç”Ÿæˆ]
    B --> C[self._update_causal_mask]
    C --> D[causal_mask: 4D tensor]
    
    %% ä¸»å¾ªç¯å¼€å§‹
    D --> E[Layerå¾ªç¯å¼€å§‹: layer_idx=0,1,2,...]
    
    %% åˆ†æ”¯1: layer_idx == 0 (ç¬¬ä¸€å±‚)
    E --> F{layer_idx == 0?}
    F -->|Yes| G[ç¬¬ä¸€å±‚: æ— é‡è®¡ç®—]
    G --> H[DecoderLayer - æ ‡å‡†è·¯å¾„]
    
    %% åˆ†æ”¯2: layer_idx > 0 (åç»­å±‚)
    F -->|No| I[åç»­å±‚: éœ€è¦é‡è®¡ç®—]
    I --> J[_recompute_previous_mlp_outputs]
    
    %% é‡è®¡ç®—åˆ†æ”¯è¯¦ç»†å±•å¼€
    J --> K[éå†å‰é¢æ‰€æœ‰å±‚ i=0 to layer_idx-1]
    K --> L[è·å–stored_weights i]
    L --> M[forward_with_precomputed_weights]
    M --> N[ğŸ”¥ causal_maské‡æ–°åº”ç”¨]
    N --> O[è¿”å›é‡è®¡ç®—çš„MLPè¾“å‡º]
    O --> P[DecoderLayer - é‡è®¡ç®—è·¯å¾„]
    
    %% ä¸¤æ¡è·¯å¾„æ±‡åˆ
    H --> Q[å­˜å‚¨å½“å‰å±‚æƒé‡]
    P --> Q
    Q --> R[æ›´æ–°stored_weightsåˆ—è¡¨]
    R --> S{æ›´å¤šå±‚?}
    S -->|Yes| E
    S -->|No| T[æœ€ç»ˆè¾“å‡º]
    
    %% æ ·å¼å®šä¹‰
    classDef inputNode fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef processNode fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef criticalNode fill:#ffebee,stroke:#c62828,stroke-width:3px
    classDef outputNode fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    
    class A,B inputNode
    class C,D,E,F,G,H,I,K,L,Q,R,S processNode
    class J,M,N criticalNode
    class O,P,T outputNode
```
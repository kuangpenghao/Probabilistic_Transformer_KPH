#!/usr/bin/env python
# coding=utf-8
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯Method3_2å’ŒMethod4_2ä¸­çš„å¯å­¦ä¹ æƒé‡å‚æ•°æ˜¯å¦è¢«æ­£ç¡®å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import os
from models.version3.configuration_llama_v3 import Method3_2Config_v3, Method4_2Config_v3
from models.version3.Method3_2_v3 import Method3_2LlamaForCausalLM_v3
from models.version3.Method4_2_v3 import Method4_2LlamaForCausalLM_v3


def create_test_model(model_class, config_class, model_name="Method3_2"):
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    print(f"\n=== åˆ›å»º{model_name}æµ‹è¯•æ¨¡å‹ ===")
    
    # åˆ›å»ºå°å‹é…ç½®ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
    config = config_class(
        vocab_size=1000,
        hidden_size=128,
        intermediate_size=256,
        num_hidden_layers=4,  # åªç”¨4å±‚ä¾¿äºè§‚å¯Ÿ
        num_attention_heads=4,
        max_position_embeddings=512,
        torch_dtype="float32"
    )
    
    model = model_class(config)
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œæ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    return model, config


def print_initial_weights(model, model_name):
    """æ‰“å°æ¨¡å‹åˆå§‹æƒé‡"""
    print(f"\n=== {model_name} åˆå§‹æƒé‡åˆ†å¸ƒ ===")
    
    if hasattr(model, 'get_all_layer_weights'):
        layer_weights = model.get_all_layer_weights()
        for layer_idx, weights in enumerate(layer_weights):
            if len(weights) > 0:
                weights_np = weights.detach().cpu().numpy()
                print(f"Layer {layer_idx}: {weights_np} (sum={weights_np.sum():.6f})")
    else:
        print(f"æ¨¡å‹ {model_name} æ²¡æœ‰ get_all_layer_weights æ–¹æ³•!")


def print_learnable_parameters(model, model_name):
    """æ‰“å°æ‰€æœ‰å¯å­¦ä¹ å‚æ•°"""
    print(f"\n=== {model_name} å¯å­¦ä¹ æƒé‡å‚æ•°è¯¦æƒ… ===")
    
    learnable_weight_count = 0
    for name, param in model.named_parameters():
        if 'layer_weights' in name:
            learnable_weight_count += 1
            print(f"{name}: {param.data} (requires_grad={param.requires_grad})")
    
    print(f"æ€»è®¡å¯å­¦ä¹ æƒé‡å‚æ•°: {learnable_weight_count}")
    return learnable_weight_count


def create_simple_training_data(vocab_size, config, num_samples=100):
    """åˆ›å»ºç®€å•çš„è®­ç»ƒæ•°æ®"""
    print(f"\n=== åˆ›å»ºè®­ç»ƒæ•°æ® ===")
    
    # åˆ›å»ºç®€å•çš„éšæœºtokenåºåˆ—
    sequence_length = min(64, config.max_position_embeddings//8)  # ä½¿ç”¨è¾ƒçŸ­åºåˆ—
    
    input_ids = []
    for i in range(num_samples):
        # åˆ›å»ºéšæœºtokenåºåˆ—ï¼Œé¿å…padding token (0)
        sequence = torch.randint(1, vocab_size-1, (sequence_length,))
        input_ids.append(sequence)
    
    # å †å æˆbatch
    input_ids = torch.stack(input_ids)
    
    print(f"åˆ›å»ºäº† {num_samples} ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œåºåˆ—é•¿åº¦: {sequence_length}")
    return {"input_ids": input_ids}


def simple_training_step(model, data, optimizer, num_steps=50):
    """æ‰§è¡Œç®€å•çš„è®­ç»ƒæ­¥éª¤"""
    print(f"\n=== å¼€å§‹è®­ç»ƒ ({num_steps} æ­¥) ===")
    
    model.train()
    initial_loss = None
    final_loss = None
    
    for step in range(num_steps):
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs = model(input_ids=data['input_ids'], labels=data['input_ids'])
        loss = outputs.loss
        
        if step == 0:
            initial_loss = loss.item()
        if step == num_steps - 1:
            final_loss = loss.item()
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        if step % 10 == 0:
            print(f"Step {step}: Loss = {loss.item():.4f}")
    
    print(f"è®­ç»ƒå®Œæˆ! åˆå§‹æŸå¤±: {initial_loss:.4f}, æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
    return initial_loss, final_loss


def test_gradient_flow(model, data, model_name):
    """æµ‹è¯•æ¢¯åº¦æµæ˜¯å¦æ­£å¸¸"""
    print(f"\n=== æµ‹è¯• {model_name} æ¢¯åº¦æµ ===")
    
    model.train()
    model.zero_grad()
    
    # å‰å‘ä¼ æ’­
    outputs = model(input_ids=data['input_ids'], labels=data['input_ids'])
    loss = outputs.loss
    
    print(f"å‰å‘ä¼ æ’­æŸå¤±: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥å¯å­¦ä¹ æƒé‡çš„æ¢¯åº¦
    gradient_found = False
    for name, param in model.named_parameters():
        if 'layer_weights' in name:
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                print(f"{name} æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
                if grad_norm > 1e-8:
                    gradient_found = True
            else:
                print(f"{name} æ²¡æœ‰æ¢¯åº¦!")
    
    if gradient_found:
        print("âœ… å‘ç°å¯å­¦ä¹ æƒé‡çš„æ¢¯åº¦ï¼Œæ¢¯åº¦æµæ­£å¸¸")
    else:
        print("âŒ æ²¡æœ‰å‘ç°å¯å­¦ä¹ æƒé‡çš„æ¢¯åº¦ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜!")
    
    return gradient_found


def compare_weights_before_after(model, model_name, initial_weights, final_weights):
    """æ¯”è¾ƒè®­ç»ƒå‰åçš„æƒé‡å˜åŒ–"""
    print(f"\n=== {model_name} æƒé‡å˜åŒ–åˆ†æ ===")
    
    changed_layers = 0
    total_change = 0.0
    
    for layer_idx in range(len(initial_weights)):
        if len(initial_weights[layer_idx]) > 0 and len(final_weights[layer_idx]) > 0:
            init_weights = initial_weights[layer_idx]
            final_weights_layer = final_weights[layer_idx]
            
            # è®¡ç®—æƒé‡å˜åŒ–
            weight_change = torch.abs(final_weights_layer - init_weights).sum().item()
            total_change += weight_change
            
            print(f"Layer {layer_idx}:")
            print(f"  åˆå§‹: {init_weights.numpy()}")
            print(f"  æœ€ç»ˆ: {final_weights_layer.numpy()}")
            print(f"  å˜åŒ–: {weight_change:.6f}")
            
            if weight_change > 1e-6:
                changed_layers += 1
    
    print(f"\næ€»ç»“:")
    print(f"  å˜åŒ–çš„å±‚æ•°: {changed_layers}/{len(initial_weights)}")
    print(f"  æ€»æƒé‡å˜åŒ–: {total_change:.6f}")
    
    if total_change > 1e-4:
        print("âœ… æƒé‡å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œå­¦ä¹ æ­£å¸¸")
        return True
    else:
        print("âŒ æƒé‡å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½å­˜åœ¨å­¦ä¹ é—®é¢˜!")
        return False


def save_test_results(results, output_dir="test_results"):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    results_file = os.path.join(output_dir, "learnable_weights_test_results.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")


def main():
    print("å¼€å§‹å¯å­¦ä¹ æƒé‡å‚æ•°æµ‹è¯•...")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    test_results = {}
    
    # æµ‹è¯•Method3_2
    print("\n" + "="*60)
    print("æµ‹è¯• Method3_2 (MLPå¯å­¦ä¹ æƒé‡)")
    print("="*60)
    
    model3_2, config3_2 = create_test_model(Method3_2LlamaForCausalLM_v3, Method3_2Config_v3, "Method3_2")
    
    # æ£€æŸ¥å¯å­¦ä¹ å‚æ•°
    learnable_count_3_2 = print_learnable_parameters(model3_2, "Method3_2")
    
    # æ‰“å°åˆå§‹æƒé‡
    print_initial_weights(model3_2, "Method3_2")
    initial_weights_3_2 = model3_2.get_all_layer_weights()
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®
    train_data = create_simple_training_data(config3_2.vocab_size, config3_2)
    
    # æµ‹è¯•æ¢¯åº¦æµ
    gradient_ok_3_2 = test_gradient_flow(model3_2, train_data, "Method3_2")
    
    # è®­ç»ƒ
    optimizer3_2 = optim.Adam(model3_2.parameters(), lr=1e-3)
    initial_loss_3_2, final_loss_3_2 = simple_training_step(model3_2, train_data, optimizer3_2)
    
    # è·å–è®­ç»ƒåæƒé‡
    final_weights_3_2 = model3_2.get_all_layer_weights()
    print_initial_weights(model3_2, "Method3_2 (è®­ç»ƒå)")
    
    # æ¯”è¾ƒæƒé‡å˜åŒ–
    weights_changed_3_2 = compare_weights_before_after(model3_2, "Method3_2", initial_weights_3_2, final_weights_3_2)
    
    test_results["Method3_2"] = {
        "learnable_parameters_count": learnable_count_3_2,
        "gradient_flow_ok": gradient_ok_3_2,
        "initial_loss": initial_loss_3_2,
        "final_loss": final_loss_3_2,
        "loss_decreased": final_loss_3_2 < initial_loss_3_2,
        "weights_changed": weights_changed_3_2,
        "initial_weights": [w.tolist() if len(w) > 0 else [] for w in initial_weights_3_2],
        "final_weights": [w.tolist() if len(w) > 0 else [] for w in final_weights_3_2]
    }
    
    # æµ‹è¯•Method4_2
    print("\n" + "="*60)
    print("æµ‹è¯• Method4_2 (Attentionå¯å­¦ä¹ æƒé‡)")
    print("="*60)
    
    model4_2, config4_2 = create_test_model(Method4_2LlamaForCausalLM_v3, Method4_2Config_v3, "Method4_2")
    
    # æ£€æŸ¥å¯å­¦ä¹ å‚æ•°
    learnable_count_4_2 = print_learnable_parameters(model4_2, "Method4_2")
    
    # æ‰“å°åˆå§‹æƒé‡
    print_initial_weights(model4_2, "Method4_2")
    initial_weights_4_2 = model4_2.get_all_layer_weights()
    
    # åˆ›å»ºè®­ç»ƒæ•°æ® (ä½¿ç”¨ç›¸åŒçš„æ•°æ®)
    
    # æµ‹è¯•æ¢¯åº¦æµ
    gradient_ok_4_2 = test_gradient_flow(model4_2, train_data, "Method4_2")
    
    # è®­ç»ƒ
    optimizer4_2 = optim.Adam(model4_2.parameters(), lr=1e-3)
    initial_loss_4_2, final_loss_4_2 = simple_training_step(model4_2, train_data, optimizer4_2)
    
    # è·å–è®­ç»ƒåæƒé‡
    final_weights_4_2 = model4_2.get_all_layer_weights()
    print_initial_weights(model4_2, "Method4_2 (è®­ç»ƒå)")
    
    # æ¯”è¾ƒæƒé‡å˜åŒ–
    weights_changed_4_2 = compare_weights_before_after(model4_2, "Method4_2", initial_weights_4_2, final_weights_4_2)
    
    test_results["Method4_2"] = {
        "learnable_parameters_count": learnable_count_4_2,
        "gradient_flow_ok": gradient_ok_4_2,
        "initial_loss": initial_loss_4_2,
        "final_loss": final_loss_4_2,
        "loss_decreased": final_loss_4_2 < initial_loss_4_2,
        "weights_changed": weights_changed_4_2,
        "initial_weights": [w.tolist() if len(w) > 0 else [] for w in initial_weights_4_2],
        "final_weights": [w.tolist() if len(w) > 0 else [] for w in final_weights_4_2]
    }
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    save_test_results(test_results)
    
    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    for model_name, results in test_results.items():
        print(f"\n{model_name}:")
        print(f"  âœ… å¯å­¦ä¹ å‚æ•°æ•°é‡: {results['learnable_parameters_count']}")
        print(f"  {'âœ…' if results['gradient_flow_ok'] else 'âŒ'} æ¢¯åº¦æµ: {'æ­£å¸¸' if results['gradient_flow_ok'] else 'å¼‚å¸¸'}")
        print(f"  {'âœ…' if results['loss_decreased'] else 'âŒ'} æŸå¤±ä¸‹é™: {results['initial_loss']:.4f} â†’ {results['final_loss']:.4f}")
        print(f"  {'âœ…' if results['weights_changed'] else 'âŒ'} æƒé‡å˜åŒ–: {'æ˜¾è‘—' if results['weights_changed'] else 'å¾®å°'}")
    
    # æ£€æŸ¥æ•´ä½“ç»“æœ
    all_tests_passed = all(
        results['gradient_flow_ok'] and results['loss_decreased'] and results['weights_changed']
        for results in test_results.values()
    )
    
    if all_tests_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å¯å­¦ä¹ æƒé‡å·¥ä½œæ­£å¸¸")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
    
    return test_results


if __name__ == "__main__":
    results = main()

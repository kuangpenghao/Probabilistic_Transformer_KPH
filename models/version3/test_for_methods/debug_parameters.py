#!/usr/bin/env python
# coding=utf-8
"""
è°ƒè¯•è„šæœ¬ï¼šæ£€æŸ¥Method3_2å’ŒMethod4_2çš„å‚æ•°æ³¨å†Œæƒ…å†µ
"""

import torch
from models.version3.configuration_llama_v3 import Method3_2Config_v3, Method4_2Config_v3
from models.version3.Method3_2_v3 import Method3_2LlamaForCausalLM_v3
from models.version3.Method4_2_v3 import Method4_2LlamaForCausalLM_v3


def debug_model_parameters(model, model_name):
    """è°ƒè¯•æ¨¡å‹å‚æ•°"""
    print(f"\n=== {model_name} å‚æ•°è°ƒè¯• ===")
    
    # æ£€æŸ¥æ‰€æœ‰å‘½åå‚æ•°
    print("\næ‰€æœ‰å‚æ•°:")
    total_params = 0
    layer_weight_params = 0
    
    for name, param in model.named_parameters():
        total_params += 1
        if 'layer_weights' in name:
            layer_weight_params += 1
            print(f"  ğŸ“Œ {name}: {param.shape}, requires_grad={param.requires_grad}")
            print(f"     å€¼: {param.data}")
        elif 'weight' in name and len(name.split('.')) < 6:  # åªæ˜¾ç¤ºä¸»è¦æƒé‡
            print(f"  ğŸ”§ {name}: {param.shape}")
    
    print(f"\næ€»å‚æ•°æ•°: {total_params}")
    print(f"å¯å­¦ä¹ æƒé‡å‚æ•°æ•°: {layer_weight_params}")
    
    # æ£€æŸ¥layersæ¨¡å—
    print(f"\næ¨¡å‹ç»“æ„:")
    if hasattr(model, 'model') and hasattr(model.model, 'layers'):
        layers = model.model.layers
        print(f"  å±‚æ•°: {len(layers)}")
        
        for i, layer in enumerate(layers):
            print(f"\n  Layer {i}:")
            
            # æ£€æŸ¥MLPæ®‹å·®è¿æ¥å™¨
            if hasattr(layer, 'modified_residual_mlp'):
                mlp_residual = layer.modified_residual_mlp
                print(f"    MLPæ®‹å·®è¿æ¥å™¨: {type(mlp_residual).__name__}")
                print(f"    layer_idx: {mlp_residual.layer_idx}")
                
                if hasattr(mlp_residual, 'layer_weights'):
                    print(f"    layer_weights: {mlp_residual.layer_weights}")
                    print(f"    layer_weights.shape: {mlp_residual.layer_weights.shape}")
                    print(f"    layer_weights.requires_grad: {mlp_residual.layer_weights.requires_grad}")
                else:
                    print(f"    âŒ æ²¡æœ‰layer_weightså±æ€§!")
            
            # æ£€æŸ¥Attentionæ®‹å·®è¿æ¥å™¨
            if hasattr(layer, 'modified_residual_attn'):
                attn_residual = layer.modified_residual_attn
                print(f"    Attentionæ®‹å·®è¿æ¥å™¨: {type(attn_residual).__name__}")
                print(f"    layer_idx: {attn_residual.layer_idx}")
                
                if hasattr(attn_residual, 'layer_weights'):
                    print(f"    layer_weights: {attn_residual.layer_weights}")
                    print(f"    layer_weights.shape: {attn_residual.layer_weights.shape}")
                    print(f"    layer_weights.requires_grad: {attn_residual.layer_weights.requires_grad}")
                else:
                    print(f"    âŒ æ²¡æœ‰layer_weightså±æ€§!")


def test_forward_pass(model, model_name):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print(f"\n=== {model_name} å‰å‘ä¼ æ’­æµ‹è¯• ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 8
    vocab_size = model.config.vocab_size
    
    input_ids = torch.randint(1, vocab_size-1, (batch_size, seq_len))
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(input_ids=input_ids)
        print(f"è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{outputs.logits.min():.3f}, {outputs.logits.max():.3f}]")
    
    # æ£€æŸ¥æƒé‡
    if hasattr(model, 'get_all_layer_weights'):
        weights = model.get_all_layer_weights()
        print(f"æƒé‡å±‚æ•°: {len(weights)}")
        for i, w in enumerate(weights):
            if len(w) > 0:
                print(f"  Layer {i}: {w}")


def main():
    print("å¼€å§‹æ¨¡å‹å‚æ•°è°ƒè¯•...")
    
    # åˆ›å»ºå°å‹é…ç½®
    config3_2 = Method3_2Config_v3(
        vocab_size=100,
        hidden_size=64,
        intermediate_size=128,
        num_hidden_layers=3,
        num_attention_heads=2,
        max_position_embeddings=128,
        torch_dtype="float32"
    )
    
    config4_2 = Method4_2Config_v3(
        vocab_size=100,
        hidden_size=64,
        intermediate_size=128,
        num_hidden_layers=3,
        num_attention_heads=2,
        max_position_embeddings=128,
        torch_dtype="float32"
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºMethod3_2æ¨¡å‹...")
    model3_2 = Method3_2LlamaForCausalLM_v3(config3_2)
    
    print("åˆ›å»ºMethod4_2æ¨¡å‹...")
    model4_2 = Method4_2LlamaForCausalLM_v3(config4_2)
    
    # è°ƒè¯•å‚æ•°
    debug_model_parameters(model3_2, "Method3_2")
    debug_model_parameters(model4_2, "Method4_2")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_forward_pass(model3_2, "Method3_2")
    test_forward_pass(model4_2, "Method4_2")


if __name__ == "__main__":
    main()

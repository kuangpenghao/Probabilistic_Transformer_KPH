# Method1_v3 Causal Maskä»£ç è·¯å¾„è¿½è¸ªè¡¨

## ğŸ” å®Œæ•´ä»£ç è°ƒç”¨è·¯å¾„è¡¨

| é˜¶æ®µ | æ–‡ä»¶ä½ç½® | è¡Œæ•° | å‡½æ•°/æ–¹æ³• | å…³é”®æ“ä½œ | ä¼ é€’å‚æ•° |
|------|----------|------|-----------|----------|----------|
| **ç”Ÿæˆé˜¶æ®µ** |
| 1 | Method1_v3.py | 367-369 | `Method1LlamaModel_v3.forward()` | `self._update_causal_mask()` | `attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions` |
| 2 | ç»§æ‰¿è‡ªLlamaModel | - | `_update_causal_mask()` | ç”Ÿæˆ4D causal mask | è¿”å› `causal_mask` |
| **åˆ†å‘é˜¶æ®µ** |
| 3 | Method1_v3.py | 378-384 | `Method1LlamaModel_v3.forward()` | Layerå¾ªç¯å¼€å§‹ | `for layer_idx, decoder_layer in enumerate(...)` |
| 4 | Method1_v3.py | 383-387 | æ¡ä»¶åˆ¤æ–­ | `if layer_idx > 0:` | å†³å®šæ˜¯å¦è§¦å‘é‡è®¡ç®—è·¯å¾„ |
| **æ ‡å‡†è·¯å¾„** |
| 5 | Method1_v3.py | 411-423 | `decoder_layer()` è°ƒç”¨ | ä¼ é€’causal_mask | `attention_mask=causal_mask` |
| 6 | Method1_v3.py | 161-172 | `Method1DecoderLayer_v3.forward()` | è°ƒç”¨self_attn | `attention_mask=attention_mask` |
| 7 | Method1_v3.py | 37-47 | `Method1LlamaAttention_v3.forward()` | è°ƒç”¨çˆ¶ç±» | `attention_mask=attention_mask` |
| 8 | ç»§æ‰¿è‡ªLlamaAttention | - | `LlamaAttention.forward()` | æ ‡å‡†causalä¿æŠ¤ | âœ… å®‰å…¨åº”ç”¨ |
| **é‡è®¡ç®—è·¯å¾„** |
| 9 | Method1_v3.py | 383-387 | `_recompute_previous_mlp_outputs()` è°ƒç”¨ | ä¼ é€’causal_mask | `causal_mask, position_ids, cache_position` |
| 10 | Method1_v3.py | 291-298 | `_recompute_previous_mlp_outputs()` | è°ƒç”¨ä¿®å¤æ–¹æ³• | `attention_mask=attention_mask` |
| 11 | Method1_v3.py | 96-118 | `forward_with_precomputed_weights()` | ä¿®å¤é€»è¾‘ | `apply_strict_causal_mask=True` |
| 12 | Method1_v3.py | 103-111 | causal maské‡æ–°åº”ç”¨ | `masked_weights = trimmed_attn_weights + causal_mask` | âœ… ä¿®å¤å®Œæˆ |
| **æ±‡åˆé˜¶æ®µ** |
| 13 | Method1_v3.py | 425-427 | è·å–layerè¾“å‡º | `hidden_states = layer_outputs[0]` | åˆå¹¶ä¸¤è·¯å¾„ç»“æœ |
| 14 | Method1_v3.py | 432-433 | å­˜å‚¨æƒé‡ | `stored_weights.append(current_weights)` | ä¸ºä¸‹ä¸€å±‚å‡†å¤‡ |

## ğŸ¯ å…³é”®ä»£ç ç‰‡æ®µå®šä½

### 1. Causal Maskç”Ÿæˆ (ç¬¬367-369è¡Œ)
```python
causal_mask = self._update_causal_mask(
    attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
)
```

### 2. é‡è®¡ç®—è·¯å¾„è§¦å‘ (ç¬¬383-387è¡Œ)
```python
if layer_idx > 0:
    recomputed_mlp_outputs = self._recompute_previous_mlp_outputs(
        hidden_states, stored_weights, layer_idx, position_embeddings, 
        causal_mask, position_ids, cache_position  # ğŸ”¥ å…³é”®ä¼ é€’
    )
```

### 3. æ ‡å‡†è·¯å¾„ä¼ é€’ (ç¬¬411è¡Œ)
```python
layer_outputs = decoder_layer(
    hidden_states,
    attention_mask=causal_mask,  # ğŸ”¥ æ ‡å‡†ä¼ é€’
    # ...å…¶ä»–å‚æ•°
)
```

### 4. é‡è®¡ç®—ä¸­çš„ä¼ é€’ (ç¬¬291-298è¡Œ)
```python
attn_output = layer.self_attn.forward_with_precomputed_weights(
    hidden_states=normalized_input,
    attn_weights=attn_weights,
    v_proj_weight=v_proj_weight,
    attention_mask=attention_mask,  # ğŸ”¥ å…³é”®ä¼ é€’
    position_ids=position_ids,
    cache_position=cache_position,
    apply_strict_causal_mask=True,  # ğŸ”¥ å¯ç”¨ä¿®å¤
)
```

### 5. ä¿®å¤é€»è¾‘æ ¸å¿ƒ (ç¬¬96-118è¡Œ)
```python
if apply_strict_causal_mask and attention_mask is not None:
    # è·å–å½“å‰åºåˆ—çš„mask
    current_seq_len = hidden_states.shape[1]
    target_len = attn_weights.shape[-1]
    
    # ç¡®ä¿ç»´åº¦åŒ¹é…
    if current_seq_len <= target_len:
        # è£å‰ªmaskåˆ°å½“å‰åºåˆ—é•¿åº¦
        causal_mask = attention_mask[:, :, :current_seq_len, :target_len]
        
        # é‡æ–°åº”ç”¨causal mask
        masked_weights = trimmed_attn_weights + causal_mask  # ğŸ”¥ æ ¸å¿ƒä¿®å¤
        attn_weights_final = F.softmax(masked_weights, dim=-1, dtype=attn_weights.dtype)
```

## ğŸ“Š ä¼ é€’è·¯å¾„ç»Ÿè®¡

| è·¯å¾„ç±»å‹ | è°ƒç”¨æ·±åº¦ | å…³é”®èŠ‚ç‚¹æ•° | å®‰å…¨æ£€æŸ¥ç‚¹ |
|----------|----------|------------|------------|
| æ ‡å‡†è·¯å¾„ | 4å±‚ | 5ä¸ª | 1ä¸ª (çˆ¶ç±»ä¸­) |
| é‡è®¡ç®—è·¯å¾„ | 6å±‚ | 7ä¸ª | 2ä¸ª (ä¿®å¤æ–¹æ³•ä¸­) |
| æ€»è®¡ | - | 12ä¸ª | 3ä¸ª |

## ğŸ›¡ï¸ å®‰å…¨æ£€æŸ¥ç‚¹è¯¦ç»†

### æ£€æŸ¥ç‚¹1: æ ‡å‡†è·¯å¾„ (LlamaAttention.forward)
- **ä½ç½®**: ç»§æ‰¿çš„çˆ¶ç±»æ–¹æ³•
- **æ£€æŸ¥å†…å®¹**: æ ‡å‡†causal maskåº”ç”¨
- **ä¿æŠ¤çº§åˆ«**: âœ… å®Œå…¨å®‰å…¨

### æ£€æŸ¥ç‚¹2: é‡è®¡ç®—è·¯å¾„ç»´åº¦æ£€æŸ¥ (ç¬¬102-119è¡Œ)
- **ä½ç½®**: `forward_with_precomputed_weights()`
- **æ£€æŸ¥å†…å®¹**: åºåˆ—é•¿åº¦å…¼å®¹æ€§
- **ä¿æŠ¤çº§åˆ«**: âœ… ç»´åº¦å®‰å…¨

### æ£€æŸ¥ç‚¹3: é‡è®¡ç®—è·¯å¾„maské‡æ–°åº”ç”¨ (ç¬¬109-111è¡Œ)
- **ä½ç½®**: `forward_with_precomputed_weights()`
- **æ£€æŸ¥å†…å®¹**: causal maské‡æ–°åº”ç”¨å’Œå½’ä¸€åŒ–
- **ä¿æŠ¤çº§åˆ«**: âœ… ä¿¡æ¯æ³„æ¼ä¿®å¤

## ğŸ”„ æ•°æ®æµå‘å›¾ (ç®€åŒ–ç‰ˆ)

```
è¾“å…¥æ•°æ®
    â†“
causal_maskç”Ÿæˆ (L367)
    â†“
layer_idxåˆ¤æ–­
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   â”‚
â–¼                   â–¼
æ ‡å‡†è·¯å¾„            é‡è®¡ç®—è·¯å¾„
DecoderLayer        _recompute_previous_mlp_outputs
(L411)              (L383)
    â†“                   â†“
self_attn           forward_with_precomputed_weights
(L161)              (L291)
    â†“                   â†“
super().forward     apply_strict_causal_mask
(L37)               (L96)
    â†“                   â†“
âœ… å®‰å…¨              âœ… ä¿®å¤å®Œæˆ
    â”‚                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        layer_outputsåˆå¹¶
              â†“
        stored_weightsæ›´æ–°
              â†“
        ä¸‹ä¸€å±‚æˆ–ç»“æŸ
```

## ğŸ¯ æ€»ç»“

Method1_v3çš„causal maskä¼ é€’è·¯å¾„ç°åœ¨å…·å¤‡ï¼š

1. **å•ä¸€æƒå¨æº**: æ‰€æœ‰maskéƒ½æ¥è‡ªç¬¬367è¡Œçš„ç»Ÿä¸€ç”Ÿæˆ
2. **åŒè·¯å¾„å®Œæ•´ä¿æŠ¤**: æ ‡å‡†è·¯å¾„å’Œé‡è®¡ç®—è·¯å¾„éƒ½æœ‰å®Œæ•´çš„causalä¿æŠ¤
3. **å¤šå±‚å®‰å…¨æ£€æŸ¥**: ä»ç”Ÿæˆåˆ°åº”ç”¨æœ‰3ä¸ªç‹¬ç«‹çš„å®‰å…¨æ£€æŸ¥ç‚¹
4. **é—­ç¯éªŒè¯**: æ¯ä¸ªå…³é”®èŠ‚ç‚¹éƒ½æœ‰å¯¹åº”çš„éªŒè¯æœºåˆ¶

è¿™ç¡®ä¿äº†Method1_v3åœ¨ä¿æŒç®—æ³•åˆ›æ–°çš„åŒæ—¶ï¼Œå…·å¤‡ä¸æ ‡å‡†LLaMAå®Œå…¨ç›¸åŒçš„å› æœå®‰å…¨æ€§ã€‚

#!/usr/bin/env python3
"""
æµ‹è¯•Method1_v3çš„Oæƒé‡ä¼˜åŒ–æ•ˆæœ
éªŒè¯æ–°å¢çš„Oæƒé‡ä¿å­˜å’Œå¤ç”¨æœºåˆ¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import torch.nn.functional as F
import warnings
from typing import Optional


def test_o_proj_weight_optimization():
    """æµ‹è¯•Oæƒé‡å’ŒOåç½®ä¼˜åŒ–æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("ğŸ” æµ‹è¯•Oæƒé‡å’ŒOåç½®ä¼˜åŒ–æ•ˆæœ...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, hidden_size = 2, 8, 512
    num_heads = 8
    head_dim = hidden_size // num_heads
    
    # æ¨¡æ‹Ÿattention weightsã€hidden stateså’Œæƒé‡çŸ©é˜µ
    torch.manual_seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
    attn_weights = torch.randn(batch_size, num_heads, seq_len, seq_len)
    attn_weights = F.softmax(attn_weights, dim=-1)
    
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    
    # æ¨¡æ‹ŸVå’ŒOæƒé‡çŸ©é˜µä»¥åŠåç½®
    v_proj_weight = torch.randn(hidden_size, hidden_size)
    o_proj_weight = torch.randn(hidden_size, hidden_size)
    o_proj_bias = torch.randn(hidden_size)  # æ–°å¢ï¼šOæŠ•å½±åç½®
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Sequence length: {seq_len}")
    print(f"   - Hidden size: {hidden_size}")
    print(f"   - Number of heads: {num_heads}")
    print(f"   - Head dimension: {head_dim}")
    print(f"   - O projection bias: {o_proj_bias.shape}")
    
    # æ¨¡æ‹ŸåŸæœ‰å®ç°ï¼ˆä¸ä½¿ç”¨é¢„è®¡ç®—çš„Oæƒé‡å’Œbiasï¼‰
    print("\nğŸ”´ åŸæœ‰å®ç°ï¼ˆé‡æ–°è®¡ç®—OæŠ•å½±ï¼‰:")
    
    # è®¡ç®—V
    value_states = F.linear(hidden_states, v_proj_weight)
    value_states = value_states.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
    
    # Attentionè®¡ç®—
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    attn_output = attn_output.reshape(batch_size, seq_len, hidden_size)
    
    # é‡æ–°è®¡ç®—OæŠ•å½±ï¼ˆåŒ…å«biasï¼‰
    output_old = F.linear(attn_output, o_proj_weight, bias=o_proj_bias)
    
    print(f"   - Vè®¡ç®—: é‡æ–°è®¡ç®—")
    print(f"   - OæŠ•å½±: é‡æ–°è®¡ç®—ï¼ˆåŒ…å«æƒé‡å’Œåç½®ï¼‰")
    print(f"   - è¾“å‡ºå½¢çŠ¶: {output_old.shape}")
    
    # æ¨¡æ‹Ÿæ–°å®ç°ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„Oæƒé‡å’Œbiasï¼‰
    print("\nğŸŸ¢ æ–°å®ç°ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„Oæƒé‡å’Œåç½®ï¼‰:")
    
    # è®¡ç®—Vï¼ˆé‡æ–°è®¡ç®—ï¼Œå› ä¸ºè¾“å…¥å˜äº†ï¼‰
    value_states_new = F.linear(hidden_states, v_proj_weight)
    value_states_new = value_states_new.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
    
    # Attentionè®¡ç®—
    attn_output_new = torch.matmul(attn_weights, value_states_new)
    attn_output_new = attn_output_new.transpose(1, 2).contiguous()
    attn_output_new = attn_output_new.reshape(batch_size, seq_len, hidden_size)
    
    # ä½¿ç”¨é¢„è®¡ç®—çš„Oæƒé‡å’Œåç½®
    output_new = F.linear(attn_output_new, o_proj_weight, bias=o_proj_bias)
    
    print(f"   - Vè®¡ç®—: é‡æ–°è®¡ç®—")
    print(f"   - OæŠ•å½±: ä½¿ç”¨é¢„è®¡ç®—æƒé‡å’Œåç½®")
    print(f"   - è¾“å‡ºå½¢çŠ¶: {output_new.shape}")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    diff = torch.abs(output_old - output_new).max()
    print(f"\nğŸ“ˆ ä¸€è‡´æ€§éªŒè¯:")
    print(f"   - æœ€å¤§è¾“å‡ºå·®å¼‚: {diff.item():.10f}")
    print(f"   - ç»“æœä¸€è‡´æ€§: {'âœ… å®Œå…¨ä¸€è‡´' if diff < 1e-6 else 'âŒ å­˜åœ¨å·®å¼‚'}")
    
    # æ€§èƒ½åˆ†æ
    print(f"\nâš¡ æ€§èƒ½åˆ†æ:")
    print(f"   - åŸå®ç°: éœ€è¦é‡æ–°è®¡ç®—OæŠ•å½±ï¼ˆæƒé‡+åç½®ï¼‰")
    print(f"   - æ–°å®ç°: å¤ç”¨é¢„è®¡ç®—çš„Oæƒé‡å’Œåç½®")
    print(f"   - èŠ‚çœæ“ä½œ: ä¸€æ¬¡çº¿æ€§å˜æ¢ ({hidden_size}x{hidden_size}) + åç½®åŠ æ³•")
    print(f"   - å†…å­˜ä¼˜åŒ–: å¤ç”¨å·²å­˜å‚¨çš„æƒé‡çŸ©é˜µå’Œåç½®å‘é‡")
    
    return diff < 1e-6


def test_weight_storage_structure():
    """æµ‹è¯•æƒé‡å­˜å‚¨ç»“æ„æ˜¯å¦æ­£ç¡®"""
    print("\nğŸ—‚ï¸ æµ‹è¯•æƒé‡å­˜å‚¨ç»“æ„...")
    
    # æ¨¡æ‹Ÿå­˜å‚¨çš„æƒé‡ç»“æ„
    stored_weights_example = {
        'attn_weights': torch.randn(2, 8, 8, 8),  # attentionæƒé‡
        'v_proj_weight': torch.randn(512, 512),   # VæŠ•å½±æƒé‡
        'o_proj_weight': torch.randn(512, 512),   # OæŠ•å½±æƒé‡
        'o_proj_bias': torch.randn(512),          # OæŠ•å½±åç½®ï¼ˆæ–°å¢ï¼‰
        'mlp': None,  # MLPæ¨¡å—ï¼ˆplaceholderï¼‰
        'post_attention_layernorm': None  # LayerNormæ¨¡å—ï¼ˆplaceholderï¼‰
    }
    
    print("æƒé‡å­˜å‚¨ç»“æ„:")
    for key, value in stored_weights_example.items():
        if isinstance(value, torch.Tensor):
            print(f"   - {key}: {value.shape}")
        else:
            print(f"   - {key}: {type(value).__name__}")
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['attn_weights', 'v_proj_weight', 'o_proj_weight', 'o_proj_bias']
    all_present = all(field in stored_weights_example for field in required_fields)
    
    print(f"\nâœ… å¿…éœ€å­—æ®µæ£€æŸ¥:")
    for field in required_fields:
        present = field in stored_weights_example
        print(f"   - {field}: {'âœ… å­˜åœ¨' if present else 'âŒ ç¼ºå¤±'}")
    
    print(f"\nğŸ¯ å­˜å‚¨ç»“æ„: {'âœ… æ­£ç¡®' if all_present else 'âŒ ä¸å®Œæ•´'}")
    
    return all_present


def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨åˆ†æ...")
    
    # é…ç½®å‚æ•°
    hidden_size = 512
    num_layers = 12
    
    # è®¡ç®—æ¯å±‚å­˜å‚¨çš„æƒé‡å¤§å°
    attn_weights_size = 8 * 8 * 8 * 4  # batch * heads * seq * seq * float32
    v_proj_weight_size = hidden_size * hidden_size * 4  # float32
    o_proj_weight_size = hidden_size * hidden_size * 4  # float32
    o_proj_bias_size = hidden_size * 4  # float32ï¼ˆæ–°å¢ï¼‰
    
    total_per_layer = attn_weights_size + v_proj_weight_size + o_proj_weight_size + o_proj_bias_size
    total_all_layers = total_per_layer * num_layers
    
    print(f"æ¯å±‚å­˜å‚¨å¤§å°:")
    print(f"   - Attentionæƒé‡: {attn_weights_size / 1024:.1f} KB")
    print(f"   - Væƒé‡çŸ©é˜µ: {v_proj_weight_size / 1024:.1f} KB")
    print(f"   - Oæƒé‡çŸ©é˜µ: {o_proj_weight_size / 1024:.1f} KB")
    print(f"   - Oåç½®å‘é‡: {o_proj_bias_size / 1024:.1f} KB")
    print(f"   - æ¯å±‚æ€»è®¡: {total_per_layer / 1024:.1f} KB")
    
    print(f"\næ€»å†…å­˜ä½¿ç”¨ ({num_layers}å±‚):")
    print(f"   - æ€»è®¡: {total_all_layers / 1024 / 1024:.1f} MB")
    print(f"   - Oæƒé‡å æ¯”: {o_proj_weight_size * num_layers / total_all_layers * 100:.1f}%")
    print(f"   - Oåç½®å æ¯”: {o_proj_bias_size * num_layers / total_all_layers * 100:.1f}%")
    
    # æ•ˆç‡åˆ†æ
    print(f"\nâš¡ æ•ˆç‡æå‡:")
    print(f"   - é¿å…é‡å¤OæŠ•å½±è®¡ç®—ï¼ˆæƒé‡+åç½®ï¼‰")
    print(f"   - è®¡ç®—é‡å‡å°‘: ~{hidden_size * hidden_size + hidden_size} FLOPs/å±‚")
    print(f"   - å†…å­˜è®¿é—®ä¼˜åŒ–: å¤ç”¨å·²ç¼“å­˜æƒé‡å’Œåç½®")
    
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Method1_v3 Oæƒé‡å’ŒOåç½®ä¼˜åŒ–æµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test1_passed = test_o_proj_weight_optimization()
    test2_passed = test_weight_storage_structure()
    test3_passed = test_memory_usage()
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   - Oæƒé‡å’Œåç½®ä¼˜åŒ–: {'âœ… é€šè¿‡' if test1_passed else 'âŒ å¤±è´¥'}")
    print(f"   - å­˜å‚¨ç»“æ„: {'âœ… é€šè¿‡' if test2_passed else 'âŒ å¤±è´¥'}")
    print(f"   - å†…å­˜åˆ†æ: {'âœ… é€šè¿‡' if test3_passed else 'âŒ å¤±è´¥'}")
    
    overall_passed = test1_passed and test2_passed and test3_passed
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡' if overall_passed else 'âŒ å­˜åœ¨æµ‹è¯•å¤±è´¥'}")
    
    if overall_passed:
        print("\nğŸ‰ æ­å–œï¼Oæƒé‡å’ŒOåç½®ä¼˜åŒ–å·²æˆåŠŸå®æ–½ã€‚")
        print("   - ä¿æŒäº†è®¡ç®—ç»“æœçš„å®Œå…¨ä¸€è‡´æ€§")
        print("   - æé«˜äº†é‡è®¡ç®—çš„æ•ˆç‡")
        print("   - ä¼˜åŒ–äº†å†…å­˜ä½¿ç”¨æ¨¡å¼")
        print("   - å®Œæ•´ä¿å­˜å’Œå¤ç”¨äº†è¾“å‡ºæŠ•å½±çš„æƒé‡å’Œåç½®")
    else:
        print("\nâš ï¸ æ³¨æ„ï¼šéƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥å®ç°ã€‚")
    
    return overall_passed


if __name__ == "__main__":
    main()

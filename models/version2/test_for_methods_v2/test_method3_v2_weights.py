#!/usr/bin/env python3
"""
æµ‹è¯•Method3_v2ä¸­å¯å­¦ä¹ æƒé‡å‚æ•°çš„åŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/home/kuangph/hf-starter')

from models.version2.Method3_v2 import Method3DecoderLayer_v2, Method3Config_v2

def test_learnable_weights():
    """æµ‹è¯•å¯å­¦ä¹ æƒé‡å‚æ•°çš„åŠŸèƒ½"""
    
    # åˆ›å»ºé…ç½®
    config = Method3Config_v2(
        hidden_size=64,
        intermediate_size=256,
        num_hidden_layers=4,
        num_attention_heads=8,
        vocab_size=1000
    )
    
    print("æµ‹è¯•å¯å­¦ä¹ æƒé‡å‚æ•°åŠŸèƒ½...")
    print("=" * 50)
    
    # æµ‹è¯•ç¬¬0å±‚ï¼ˆä¸åº”è¯¥æœ‰æƒé‡å‚æ•°ï¼‰
    layer_0 = Method3DecoderLayer_v2(config, layer_idx=0)
    print(f"ç¬¬0å±‚æƒé‡å‚æ•°: {layer_0.mlp_residual_weights}")
    assert layer_0.mlp_residual_weights is None, "ç¬¬0å±‚ä¸åº”è¯¥æœ‰æƒé‡å‚æ•°"
    
    # æµ‹è¯•ç¬¬1å±‚ï¼ˆåº”è¯¥æœ‰2ä¸ªæƒé‡å‚æ•°ï¼šç¬¬0å±‚å’Œç¬¬1å±‚ï¼‰
    layer_1 = Method3DecoderLayer_v2(config, layer_idx=1)
    print(f"ç¬¬1å±‚æƒé‡å‚æ•°å½¢çŠ¶: {layer_1.mlp_residual_weights.shape}")
    print(f"ç¬¬1å±‚æƒé‡å‚æ•°å€¼: {layer_1.mlp_residual_weights}")
    assert layer_1.mlp_residual_weights.shape == (2,), "ç¬¬1å±‚åº”è¯¥æœ‰2ä¸ªæƒé‡å‚æ•°"
    
    # æµ‹è¯•ç¬¬2å±‚ï¼ˆåº”è¯¥æœ‰3ä¸ªæƒé‡å‚æ•°ï¼šç¬¬0ã€1ã€2å±‚ï¼‰
    layer_2 = Method3DecoderLayer_v2(config, layer_idx=2)
    print(f"ç¬¬2å±‚æƒé‡å‚æ•°å½¢çŠ¶: {layer_2.mlp_residual_weights.shape}")
    print(f"ç¬¬2å±‚æƒé‡å‚æ•°å€¼: {layer_2.mlp_residual_weights}")
    assert layer_2.mlp_residual_weights.shape == (3,), "ç¬¬2å±‚åº”è¯¥æœ‰3ä¸ªæƒé‡å‚æ•°"
    
    # æµ‹è¯•æƒé‡å½’ä¸€åŒ–
    print("\næµ‹è¯•æƒé‡å½’ä¸€åŒ–:")
    layer_2.mlp_residual_weights.data = torch.tensor([2.0, 3.0, 1.0])
    normalized_weights = F.softmax(layer_2.mlp_residual_weights, dim=0)
    print(f"åŸå§‹æƒé‡: {layer_2.mlp_residual_weights}")
    print(f"å½’ä¸€åŒ–åæƒé‡: {normalized_weights}")
    print(f"æƒé‡æ€»å’Œ: {normalized_weights.sum()}")
    assert torch.allclose(normalized_weights.sum(), torch.tensor(1.0)), "æƒé‡æ€»å’Œåº”è¯¥ä¸º1"
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    
    # åˆ›å»ºé…ç½®
    config = Method3Config_v2(
        hidden_size=64,
        intermediate_size=256,
        num_hidden_layers=4,
        num_attention_heads=8,
        vocab_size=1000
    )
    
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_length = 2, 10
    hidden_states = torch.randn(batch_size, seq_length, config.hidden_size)
    
    # åˆ›å»ºå±‚
    layer_2 = Method3DecoderLayer_v2(config, layer_idx=2)
    
    # æ¨¡æ‹Ÿå‰é¢å±‚çš„MLPè¾“å‡º
    previous_mlp_outputs = [
        torch.randn(batch_size, seq_length, config.hidden_size),  # ç¬¬0å±‚MLPè¾“å‡º
        torch.randn(batch_size, seq_length, config.hidden_size),  # ç¬¬1å±‚MLPè¾“å‡º
    ]
    
    # å‰å‘ä¼ æ’­
    outputs = layer_2(
        hidden_states=hidden_states,
        previous_mlp_outputs=previous_mlp_outputs
    )
    
    print(f"è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
    print(f"è¾“å‡ºå…ƒç»„é•¿åº¦: {len(outputs)}")
    print(f"æƒé‡å‚æ•°: {layer_2.mlp_residual_weights}")
    print(f"å½’ä¸€åŒ–æƒé‡: {F.softmax(layer_2.mlp_residual_weights, dim=0)}")
    
    print("\nâœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    test_learnable_weights()
    test_forward_pass()
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

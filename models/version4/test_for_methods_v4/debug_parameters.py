#!/usr/bin/env python3

import torch
from models.version4.Method1A_v4 import Method1ALlamaForCausalLM_v4
from models.version4.configuration_llama_v4 import Method1AConfig_v4

def count_parameters_detailed(model):
    """è¯¦ç»†ç»Ÿè®¡æ¨¡å‹å‚æ•°"""
    total_params = 0
    trainable_params = 0
    
    print("=" * 80)
    print("è¯¦ç»†å‚æ•°ç»Ÿè®¡:")
    print("=" * 80)
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        if param.requires_grad:
            trainable_params += param_count
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬æ–°å¢çš„æƒé‡çŸ©é˜µå‚æ•°
        if "layer_weight_matrices" in name:
            print(f"ğŸ” æ–°å¢æƒé‡çŸ©é˜µ: {name}")
            print(f"   å½¢çŠ¶: {param.shape}")
            print(f"   å‚æ•°æ•°é‡: {param_count:,}")
            print(f"   requires_grad: {param.requires_grad}")
            print(f"   æ•°æ®ç±»å‹: {param.dtype}")
            print(f"   è®¾å¤‡: {param.device}")
            print()
    
    print("=" * 80)
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    print("=" * 80)
    
    return total_params, trainable_params

def main():
    # åˆ›å»ºæ¨¡å‹é…ç½®
    config = Method1AConfig_v4(
        vocab_size=1000,
        hidden_size=64,
        intermediate_size=256,
        num_hidden_layers=4,
        num_attention_heads=4,
        max_position_embeddings=512,
        rms_norm_eps=1e-6,
        tie_word_embeddings=False,
        pad_token_id=0,
        bos_token_id=1,
        eos_token_id=2,
    )
    
    print("åˆ›å»ºMethod1A_v4æ¨¡å‹...")
    model = Method1ALlamaForCausalLM_v4(config)
    
    # ç»Ÿè®¡å‚æ•°
    total, trainable = count_parameters_detailed(model)
    
    # æ£€æŸ¥ModifiedScalingComputationçš„å‚æ•°æ³¨å†Œ
    print("\næ£€æŸ¥ModifiedScalingComputationæ¨¡å—:")
    print("=" * 50)
    
    for layer_idx, layer in enumerate(model.model.layers):
        scaling_module = layer.self_attn.modified_scaling
        print(f"\nç¬¬{layer_idx}å±‚çš„ModifiedScalingComputation:")
        print(f"  layer_initialized: {scaling_module.layer_initialized}")
        print(f"  layer_weight_matricesç±»å‹: {type(scaling_module.layer_weight_matrices)}")
        print(f"  layer_weight_matricesé•¿åº¦: {len(scaling_module.layer_weight_matrices)}")
        
        # æ£€æŸ¥æ¯å±‚çš„ParameterList
        for i, param_list in enumerate(scaling_module.layer_weight_matrices):
            print(f"    ç¬¬{i}å±‚ParameterList: é•¿åº¦={len(param_list)}, ç±»å‹={type(param_list)}")
    
    # å¼ºåˆ¶è§¦å‘ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œçœ‹çœ‹æ˜¯å¦ä¼šåˆå§‹åŒ–æƒé‡çŸ©é˜µ
    print("\n" + "=" * 80)
    print("æ‰§è¡Œå‰å‘ä¼ æ’­ä»¥è§¦å‘æƒé‡çŸ©é˜µåˆå§‹åŒ–...")
    print("=" * 80)
    
    input_ids = torch.randint(0, 1000, (2, 32))
    labels = input_ids.clone()
    
    with torch.no_grad():
        outputs = model(input_ids=input_ids, labels=labels)
    
    print(f"å‰å‘ä¼ æ’­å®Œæˆï¼ŒæŸå¤±: {outputs.loss:.4f}")
    
    # å†æ¬¡æ£€æŸ¥å‚æ•°
    print("\nå‰å‘ä¼ æ’­åçš„å‚æ•°ç»Ÿè®¡:")
    total_after, trainable_after = count_parameters_detailed(model)
    
    print(f"\nå‚æ•°å˜åŒ–:")
    print(f"  å‰å‘ä¼ æ’­å‰: {total:,} æ€»å‚æ•°, {trainable:,} å¯è®­ç»ƒå‚æ•°")
    print(f"  å‰å‘ä¼ æ’­å: {total_after:,} æ€»å‚æ•°, {trainable_after:,} å¯è®­ç»ƒå‚æ•°")
    print(f"  æ–°å¢å‚æ•°: {total_after - total:,}")

if __name__ == "__main__":
    main()

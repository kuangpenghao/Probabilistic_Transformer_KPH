#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯Method1_v4å®ç°æ˜¯å¦æ­£ç¡®éµå¾ªè®¾è®¡æ–¹æ¡ˆ

è®¾è®¡æ–¹æ¡ˆéªŒè¯è¦ç‚¹ï¼š
1. æ¯ä¸€å±‚æ³¨æ„åŠ›æœºåˆ¶ä¸­å­˜å–QK^TçŸ©é˜µ
2. v4m1_ModifiedScailingComputationç±»æ­£ç¡®å®ç°åŠ æƒè®¡ç®—
3. å°†åŸå§‹sqrt(d_k)å¹¿æ’­æˆå‘é‡ï¼Œä¸ä¹‹å‰æ‰€æœ‰å±‚QK^Tç»„æˆçš„å‘é‡è¿›è¡Œç‚¹ä¹˜
4. æ¨¡å‹ç»“æ„æ­£ç¡®è¦†å†™ï¼ˆAttention, DecoderLayer, Model, CausalLMï¼‰
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

import torch
import torch.nn as nn
import math
import numpy as np
from typing import List, Optional

# å¯¼å…¥æˆ‘ä»¬å®ç°çš„æ¨¡å‹
from models.version4.Method1_v4 import (
    Method1LlamaModel_v4, 
    Method1LlamaForCausalLM_v4, 
    Method1LlamaAttention_v4,
    Method1DecoderLayer_v4,
    v4m1_ModifiedScailingComputation
)
from models.version4.configuration_llama_v4 import Method1Config_v4

# å¯¼å…¥åŸå§‹transformersæ¨¡å‹è¿›è¡Œå¯¹æ¯”
from transformers.models.llama.modeling_llama import LlamaModel, LlamaForCausalLM
from transformers.models.llama.configuration_llama import LlamaConfig


class TestMethod1V4:
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºå°å‹é…ç½®ç”¨äºæµ‹è¯•
        self.config = Method1Config_v4(
            vocab_size=1000,
            hidden_size=128,
            intermediate_size=256,
            num_hidden_layers=4,
            num_attention_heads=4,
            num_key_value_heads=2,
            max_position_embeddings=512,
            rms_norm_eps=1e-6,
        )
        
        print(f"æµ‹è¯•é…ç½®: {self.config.num_hidden_layers}å±‚, {self.config.hidden_size}ç»´åº¦")
    
    def test_modified_scaling_computation(self):
        """æµ‹è¯•v4m1_ModifiedScailingComputationç±»"""
        print("\n=== æµ‹è¯•v4m1_ModifiedScailingComputationç±» ===")
        
        head_dim = self.config.hidden_size // self.config.num_attention_heads
        modified_scaling = v4m1_ModifiedScailingComputation(head_dim)
        
        # æµ‹è¯•åŸå§‹ç¼©æ”¾å€¼
        expected_scaling = 1.0 / math.sqrt(head_dim)
        assert abs(modified_scaling.original_scaling - expected_scaling) < 1e-6, \
            f"åŸå§‹ç¼©æ”¾å€¼é”™è¯¯: {modified_scaling.original_scaling} vs {expected_scaling}"
        print(f"âœ“ åŸå§‹ç¼©æ”¾å€¼æ­£ç¡®: {modified_scaling.original_scaling}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„QK^TçŸ©é˜µ
        batch_size, num_heads, seq_len = 2, 4, 16
        qk_shape = (batch_size, num_heads, seq_len, seq_len)
        
        # æµ‹è¯•ç¬¬0å±‚ï¼ˆåº”è¯¥ä½¿ç”¨åŸå§‹ç¼©æ”¾ï¼‰
        qk_matrix_0 = torch.randn(qk_shape)
        result_0 = modified_scaling.compute_modified_scaling([qk_matrix_0], layer_idx=0)
        expected_0 = qk_matrix_0 * expected_scaling
        
        assert torch.allclose(result_0, expected_0, atol=1e-6), "ç¬¬0å±‚ç¼©æ”¾è®¡ç®—é”™è¯¯"
        print("âœ“ ç¬¬0å±‚ç¼©æ”¾è®¡ç®—æ­£ç¡®")
        
        # æµ‹è¯•åç»­å±‚ï¼ˆåº”è¯¥ä½¿ç”¨åŠ æƒæ±‚å’Œï¼‰
        qk_matrix_1 = torch.randn(qk_shape)
        qk_matrices = [qk_matrix_0, qk_matrix_1]
        result_1 = modified_scaling.compute_modified_scaling(qk_matrices, layer_idx=1)
        
        # éªŒè¯åŠ æƒæ±‚å’Œé€»è¾‘
        expected_1 = expected_scaling * (qk_matrix_0 + qk_matrix_1)
        assert torch.allclose(result_1, expected_1, atol=1e-6), "ç¬¬1å±‚åŠ æƒç¼©æ”¾è®¡ç®—é”™è¯¯"
        print("âœ“ åç»­å±‚åŠ æƒç¼©æ”¾è®¡ç®—æ­£ç¡®")
        
        print("âœ“ v4m1_ModifiedScailingComputationç±»æµ‹è¯•é€šè¿‡")
    
    def test_attention_qk_matrix_storage(self):
        """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶ä¸­QK^TçŸ©é˜µçš„å­˜å–"""
        print("\n=== æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶QK^TçŸ©é˜µå­˜å– ===")
        
        attention = Method1LlamaAttention_v4(self.config, layer_idx=1).to(self.device)
        attention.eval()
        
        batch_size, seq_len = 2, 16
        hidden_states = torch.randn(batch_size, seq_len, self.config.hidden_size).to(self.device)
        
        # åˆ›å»ºposition_ids
        position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0).repeat(batch_size, 1)
        
        # æ¨¡æ‹Ÿä¹‹å‰å±‚çš„QK^TçŸ©é˜µ
        previous_qk_shape = (batch_size, self.config.num_attention_heads, seq_len, seq_len)
        previous_qk_matrices = [
            torch.randn(previous_qk_shape).to(self.device) for _ in range(1)
        ]
        
        with torch.no_grad():
            result = attention(
                hidden_states=hidden_states,
                position_ids=position_ids,
                previous_qk_matrices=previous_qk_matrices,
                output_attentions=True
            )
        
        # éªŒè¯è¿”å›å€¼ç»“æ„
        assert len(result) == 4, f"æ³¨æ„åŠ›è¿”å›å€¼æ•°é‡é”™è¯¯: {len(result)}"
        attn_output, attn_weights, past_key_value, current_qk_matrix = result
        
        # éªŒè¯QK^TçŸ©é˜µå½¢çŠ¶
        expected_qk_shape = (batch_size, self.config.num_attention_heads, seq_len, seq_len)
        assert current_qk_matrix.shape == expected_qk_shape, \
            f"QK^TçŸ©é˜µå½¢çŠ¶é”™è¯¯: {current_qk_matrix.shape} vs {expected_qk_shape}"
        
        print(f"âœ“ QK^TçŸ©é˜µå½¢çŠ¶æ­£ç¡®: {current_qk_matrix.shape}")
        print("âœ“ æ³¨æ„åŠ›æœºåˆ¶QK^TçŸ©é˜µå­˜å–æµ‹è¯•é€šè¿‡")
    
    def test_decoder_layer_integration(self):
        """æµ‹è¯•DecoderLayerçš„é›†æˆ"""
        print("\n=== æµ‹è¯•DecoderLayeré›†æˆ ===")
        
        decoder_layer = Method1DecoderLayer_v4(self.config, layer_idx=2).to(self.device)
        decoder_layer.eval()
        
        batch_size, seq_len = 2, 16
        hidden_states = torch.randn(batch_size, seq_len, self.config.hidden_size).to(self.device)
        
        # åˆ›å»ºposition_ids
        position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0).repeat(batch_size, 1)
        
        # æ¨¡æ‹Ÿä¹‹å‰å±‚çš„QK^TçŸ©é˜µ
        previous_qk_shape = (batch_size, self.config.num_attention_heads, seq_len, seq_len)
        previous_qk_matrices = [
            torch.randn(previous_qk_shape).to(self.device) for _ in range(2)
        ]
        
        with torch.no_grad():
            result = decoder_layer(
                hidden_states=hidden_states,
                position_ids=position_ids,
                previous_qk_matrices=previous_qk_matrices,
                output_attentions=True,
                use_cache=False
            )
        
        # éªŒè¯è¾“å‡ºç»“æ„
        assert len(result) >= 3, f"DecoderLayerè¿”å›å€¼æ•°é‡é”™è¯¯: {len(result)}"
        
        layer_output = result[0]
        current_qk_matrix = result[-1]  # æœ€åä¸€ä¸ªåº”è¯¥æ˜¯QK^TçŸ©é˜µ
        
        # éªŒè¯å±‚è¾“å‡ºå½¢çŠ¶
        assert layer_output.shape == hidden_states.shape, \
            f"å±‚è¾“å‡ºå½¢çŠ¶é”™è¯¯: {layer_output.shape} vs {hidden_states.shape}"
        
        # éªŒè¯QK^TçŸ©é˜µå½¢çŠ¶
        expected_qk_shape = (batch_size, self.config.num_attention_heads, seq_len, seq_len) 
        assert current_qk_matrix.shape == expected_qk_shape, \
            f"DecoderLayer QK^TçŸ©é˜µå½¢çŠ¶é”™è¯¯: {current_qk_matrix.shape} vs {expected_qk_shape}"
        
        print(f"âœ“ DecoderLayerè¾“å‡ºå½¢çŠ¶æ­£ç¡®: {layer_output.shape}")
        print(f"âœ“ DecoderLayer QK^TçŸ©é˜µå½¢çŠ¶æ­£ç¡®: {current_qk_matrix.shape}")
        print("âœ“ DecoderLayeré›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_full_model_forward(self):
        """æµ‹è¯•å®Œæ•´æ¨¡å‹çš„å‰å‘ä¼ æ’­"""
        print("\n=== æµ‹è¯•å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­ ===")
        
        model = Method1LlamaModel_v4(self.config).to(self.device)
        model.eval()
        
        batch_size, seq_len = 2, 16
        input_ids = torch.randint(0, self.config.vocab_size, (batch_size, seq_len)).to(self.device)
        
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                output_attentions=True,
                output_hidden_states=True
            )
        
        # éªŒè¯è¾“å‡ºç»“æ„
        assert hasattr(outputs, 'last_hidden_state'), "ç¼ºå°‘last_hidden_state"
        assert hasattr(outputs, 'attentions'), "ç¼ºå°‘attentions"
        assert hasattr(outputs, 'hidden_states'), "ç¼ºå°‘hidden_states"
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, seq_len, self.config.hidden_size)
        assert outputs.last_hidden_state.shape == expected_shape, \
            f"æœ€ç»ˆéšè—çŠ¶æ€å½¢çŠ¶é”™è¯¯: {outputs.last_hidden_state.shape} vs {expected_shape}"
        
        # éªŒè¯æ³¨æ„åŠ›æƒé‡æ•°é‡
        assert len(outputs.attentions) == self.config.num_hidden_layers, \
            f"æ³¨æ„åŠ›æƒé‡æ•°é‡é”™è¯¯: {len(outputs.attentions)} vs {self.config.num_hidden_layers}"
        
        print(f"âœ“ æ¨¡å‹è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {outputs.last_hidden_state.shape}")
        print(f"âœ“ æ³¨æ„åŠ›æƒé‡æ•°é‡æ­£ç¡®: {len(outputs.attentions)}")
        print("âœ“ å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    
    def test_causal_lm_model(self):
        """æµ‹è¯•å› æœè¯­è¨€æ¨¡å‹"""
        print("\n=== æµ‹è¯•å› æœè¯­è¨€æ¨¡å‹ ===")
        
        model = Method1LlamaForCausalLM_v4(self.config).to(self.device)
        model.eval()
        
        batch_size, seq_len = 2, 16
        input_ids = torch.randint(0, self.config.vocab_size, (batch_size, seq_len)).to(self.device)
        labels = torch.randint(0, self.config.vocab_size, (batch_size, seq_len)).to(self.device)
        
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                labels=labels,
                output_attentions=True
            )
        
        # éªŒè¯è¾“å‡ºç»“æ„
        assert hasattr(outputs, 'logits'), "ç¼ºå°‘logits"
        assert hasattr(outputs, 'loss'), "ç¼ºå°‘loss"
        
        # éªŒè¯logitså½¢çŠ¶
        expected_logits_shape = (batch_size, seq_len, self.config.vocab_size)
        assert outputs.logits.shape == expected_logits_shape, \
            f"logitså½¢çŠ¶é”™è¯¯: {outputs.logits.shape} vs {expected_logits_shape}"
        
        # éªŒè¯losså­˜åœ¨ä¸”ä¸ºæ ‡é‡
        assert outputs.loss is not None, "lossä¸åº”ä¸ºNone"
        assert outputs.loss.dim() == 0, f"lossåº”ä¸ºæ ‡é‡ï¼Œä½†ç»´åº¦ä¸º: {outputs.loss.dim()}"
        
        print(f"âœ“ Logitså½¢çŠ¶æ­£ç¡®: {outputs.logits.shape}")
        print(f"âœ“ Lossè®¡ç®—æ­£ç¡®: {outputs.loss.item():.4f}")
        print("âœ“ å› æœè¯­è¨€æ¨¡å‹æµ‹è¯•é€šè¿‡")
    
    def test_qk_matrix_accumulation_across_layers(self):
        """æµ‹è¯•QK^TçŸ©é˜µåœ¨å„å±‚é—´çš„ç´¯ç§¯ä¼ é€’"""
        print("\n=== æµ‹è¯•QK^TçŸ©é˜µè·¨å±‚ç´¯ç§¯ä¼ é€’ ===")
        
        model = Method1LlamaModel_v4(self.config).to(self.device)
        model.eval()
        
        batch_size, seq_len = 2, 8  # ä½¿ç”¨è¾ƒå°çš„åºåˆ—é•¿åº¦ä¾¿äºæµ‹è¯•
        input_ids = torch.randint(0, self.config.vocab_size, (batch_size, seq_len)).to(self.device)
        
        # æ‰‹åŠ¨æ‰§è¡Œå‰å‘ä¼ æ’­ä»¥ç›‘æ§QK^TçŸ©é˜µç´¯ç§¯
        with torch.no_grad():
            inputs_embeds = model.embed_tokens(input_ids)
            position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)
            position_embeddings = model.rotary_emb(inputs_embeds, position_ids)
            
            hidden_states = inputs_embeds
            stored_qk_matrices = []
            
            for layer_idx, decoder_layer in enumerate(model.layers):
                layer_outputs = decoder_layer(
                    hidden_states=hidden_states,
                    position_embeddings=position_embeddings,
                    previous_qk_matrices=stored_qk_matrices,
                    output_attentions=True
                )
                
                hidden_states = layer_outputs[0]
                current_qk_matrix = layer_outputs[-1]
                stored_qk_matrices.append(current_qk_matrix)
                
                # éªŒè¯QK^TçŸ©é˜µç´¯ç§¯
                assert len(stored_qk_matrices) == layer_idx + 1, \
                    f"ç¬¬{layer_idx}å±‚åï¼Œå­˜å‚¨çš„QK^TçŸ©é˜µæ•°é‡é”™è¯¯: {len(stored_qk_matrices)}"
                
                expected_qk_shape = (batch_size, self.config.num_attention_heads, seq_len, seq_len)
                assert current_qk_matrix.shape == expected_qk_shape, \
                    f"ç¬¬{layer_idx}å±‚QK^TçŸ©é˜µå½¢çŠ¶é”™è¯¯: {current_qk_matrix.shape}"
                
                print(f"âœ“ ç¬¬{layer_idx}å±‚: QK^TçŸ©é˜µæ­£ç¡®ç´¯ç§¯ï¼Œå½“å‰å…±{len(stored_qk_matrices)}ä¸ªçŸ©é˜µ")
        
        print("âœ“ QK^TçŸ©é˜µè·¨å±‚ç´¯ç§¯ä¼ é€’æµ‹è¯•é€šè¿‡")
    
    def test_scaling_modification_effect(self):
        """æµ‹è¯•ç¼©æ”¾ä¿®æ”¹çš„æ•ˆæœ"""
        print("\n=== æµ‹è¯•ç¼©æ”¾ä¿®æ”¹æ•ˆæœ ===")
        
        # åˆ›å»ºç®€åŒ–çš„æµ‹è¯•é…ç½®
        test_config = Method1Config_v4(
            vocab_size=100,
            hidden_size=64,
            intermediate_size=128,
            num_hidden_layers=3,
            num_attention_heads=2,
            num_key_value_heads=2,
            max_position_embeddings=32,
        )
        
        model = Method1LlamaModel_v4(test_config).to(self.device)
        model.eval()
        
        batch_size, seq_len = 1, 8
        input_ids = torch.randint(0, test_config.vocab_size, (batch_size, seq_len)).to(self.device)
        
        with torch.no_grad():
            # è¿è¡Œæ¨¡å‹å¹¶è·å–æ³¨æ„åŠ›æƒé‡
            outputs = model(
                input_ids=input_ids,
                output_attentions=True
            )
            
            attentions = outputs.attentions
            
            # éªŒè¯æ³¨æ„åŠ›æƒé‡çš„å½¢çŠ¶å’Œæ•°å€¼èŒƒå›´
            for layer_idx, attn_weights in enumerate(attentions):
                expected_shape = (batch_size, test_config.num_attention_heads, seq_len, seq_len)
                assert attn_weights.shape == expected_shape, \
                    f"ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›æƒé‡å½¢çŠ¶é”™è¯¯: {attn_weights.shape}"
                
                # éªŒè¯softmaxå±æ€§ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
                row_sums = attn_weights.sum(dim=-1)
                assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5), \
                    f"ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›æƒé‡ä¸æ»¡è¶³softmaxå±æ€§"
                
                # éªŒè¯æ•°å€¼èŒƒå›´ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰
                assert torch.all(attn_weights >= 0) and torch.all(attn_weights <= 1), \
                    f"ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›æƒé‡è¶…å‡º[0,1]èŒƒå›´"
                
                print(f"âœ“ ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›æƒé‡å½¢çŠ¶å’Œæ•°å€¼èŒƒå›´æ­£ç¡®")
        
        print("âœ“ ç¼©æ”¾ä¿®æ”¹æ•ˆæœæµ‹è¯•é€šè¿‡")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("å¼€å§‹è¿è¡ŒMethod1_v4æµ‹è¯•å¥—ä»¶...")
        print("=" * 60)
        
        try:
            self.test_modified_scaling_computation()
            self.test_attention_qk_matrix_storage()
            self.test_decoder_layer_integration()
            self.test_full_model_forward()
            self.test_causal_lm_model()
            self.test_qk_matrix_accumulation_across_layers()
            self.test_scaling_modification_effect()
            
            print("\n" + "=" * 60)
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Method1_v4å®ç°æ­£ç¡®éµå¾ªè®¾è®¡æ–¹æ¡ˆ")
            print("=" * 60)
            
            # æ‰“å°è®¾è®¡æ–¹æ¡ˆéªŒè¯æ€»ç»“
            print("\nè®¾è®¡æ–¹æ¡ˆéªŒè¯æ€»ç»“:")
            print("âœ“ 1. æ¯ä¸€å±‚æ³¨æ„åŠ›æœºåˆ¶ä¸­æˆåŠŸå­˜å–QK^TçŸ©é˜µ")
            print("âœ“ 2. v4m1_ModifiedScailingComputationç±»æ­£ç¡®å®ç°åŠ æƒè®¡ç®—")
            print("âœ“ 3. åŸå§‹sqrt(d_k)æ­£ç¡®å¹¿æ’­å¹¶ä¸QK^TçŸ©é˜µè¿›è¡Œç‚¹ä¹˜")
            print("âœ“ 4. æ¨¡å‹ç»“æ„æ­£ç¡®è¦†å†™(Attention, DecoderLayer, Model, CausalLM)")
            print("âœ“ 5. QK^TçŸ©é˜µåœ¨å„å±‚é—´æ­£ç¡®ç´¯ç§¯ä¼ é€’")
            print("âœ“ 6. æ³¨æ„åŠ›æƒé‡è®¡ç®—ç»“æœç¬¦åˆé¢„æœŸ")
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        return True


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    tester = TestMethod1V4()
    success = tester.run_all_tests()
    
    if success:
        print("\nâœ… æµ‹è¯•å®Œæˆï¼šMethod1_v4å®ç°ç¬¦åˆè®¾è®¡è¦æ±‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šè¯·æ£€æŸ¥å®ç°")
        sys.exit(1)

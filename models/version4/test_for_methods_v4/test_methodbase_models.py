#!/usr/bin/env python3
"""
æµ‹è¯•MethodCbaseå’ŒMethodDbaseæ¨¡å‹
"""

import torch
from models.version4.configuration_llama_v4 import MethodCbaseConfig_v4, MethodDbaseConfig_v4
from models.version4.MethodCbase_v4 import MethodCbaseLlamaForCausalLM_v4
from models.version4.MethodDbase_v4 import MethodDbaseLlamaForCausalLM_v4

def test_methodcbase():
    print("="*70)
    print("MethodCbaseæµ‹è¯• - åŸºäºåŸå§‹LlamaModel + å¯å­¦ä¹ æƒé‡åˆ—å‘é‡")
    print("="*70)
    
    # åˆ›å»ºé…ç½®
    config = MethodCbaseConfig_v4(
        vocab_size=1000,
        hidden_size=64,
        intermediate_size=128,
        num_hidden_layers=2,
        num_attention_heads=4,
        num_key_value_heads=4,
        max_position_embeddings=512,
        rms_norm_eps=1e-6,
        use_cache=True,
    )
    
    print(f"é…ç½®: hidden_size={config.hidden_size}, layers={config.num_hidden_layers}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºMethodCbaseæ¨¡å‹...")
    model = MethodCbaseLlamaForCausalLM_v4(config)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
    print(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
    
    # å‡†å¤‡è¾“å…¥
    batch_size, seq_len = 1, 8
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # Forwardå‰å‚æ•°æ•°é‡
    pre_forward_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
    print(f"Forwardå‰å‚æ•°æ•°é‡: {pre_forward_params:,}")
    
    # æ‰§è¡Œforward
    print("æµ‹è¯•å‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids)
    
    print(f"âœ… MethodCbaseå‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"   è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
    
    # Forwardåå‚æ•°æ•°é‡
    post_forward_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
    print(f"Forwardåå‚æ•°æ•°é‡: {post_forward_params:,}")
    
    if post_forward_params != pre_forward_params:
        change = post_forward_params - pre_forward_params
        print(f"âœ… åŠ¨æ€å‚æ•°åˆ›å»º: +{change:,} å‚æ•°")
    else:
        print("âš ï¸  æ— å‚æ•°å˜åŒ–ï¼ˆæ‰€æœ‰å‚æ•°åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºï¼‰")
    
    # æ£€æŸ¥æƒé‡å‘é‡
    print(f"\næ£€æŸ¥æƒé‡å‘é‡:")
    for layer_idx in range(config.num_hidden_layers):
        scaling_module = model.model.layers[layer_idx].self_attn.modified_scaling
        if scaling_module.layer_initialized[layer_idx]:
            weight_vector = scaling_module.layer_weight_vectors[layer_idx][0]
            print(f"  å±‚{layer_idx}: æƒé‡å‘é‡å½¢çŠ¶={weight_vector.shape}, å·²åˆå§‹åŒ–")
        else:
            print(f"  å±‚{layer_idx}: æœªåˆå§‹åŒ–")


def test_methoddbase():
    print("\n" + "="*70)
    print("MethodDbaseæµ‹è¯• - åŸºäºåŸå§‹LlamaModel + MLPåŠ¨æ€ç”Ÿæˆæƒé‡åˆ—å‘é‡")
    print("="*70)
    
    # åˆ›å»ºé…ç½®
    config = MethodDbaseConfig_v4(
        vocab_size=1000,
        hidden_size=64,
        intermediate_size=128,
        num_hidden_layers=2,
        num_attention_heads=4,
        num_key_value_heads=4,
        max_position_embeddings=512,
        rms_norm_eps=1e-6,
        use_cache=True,
    )
    
    print(f"é…ç½®: hidden_size={config.hidden_size}, layers={config.num_hidden_layers}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºMethodDbaseæ¨¡å‹...")
    model = MethodDbaseLlamaForCausalLM_v4(config)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
    print(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
    
    # è¯¦ç»†åˆ†æMLPå‚æ•°
    print(f"\nMLPå‚æ•°è¯¦æƒ…:")
    total_mlp_params = 0
    for layer_idx in range(config.num_hidden_layers):
        scaling_module = model.model.layers[layer_idx].self_attn.modified_scaling
        
        # MLPå‚æ•°ï¼šhidden_size -> 1 -> 1
        mlp = scaling_module.layer_mlps[layer_idx]
        mlp_params = sum(p.numel() for p in mlp.parameters())
        
        # RMSNormå‚æ•°
        norm_params = scaling_module.layer_norms[layer_idx].weight.numel()
        
        # Biaså‚æ•°
        bias_params = scaling_module.layer_biases[layer_idx].numel()
        
        layer_total = mlp_params + norm_params + bias_params
        total_mlp_params += layer_total
        
        print(f"  å±‚{layer_idx}: MLP={mlp_params}, Norm={norm_params}, Bias={bias_params}, æ€»è®¡={layer_total}")
    
    print(f"  MLPå‚æ•°æ€»è®¡: {total_mlp_params:,}")
    
    # å‡†å¤‡è¾“å…¥
    batch_size, seq_len = 1, 8
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    print(f"\nè¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # Forwardå‰å‚æ•°æ•°é‡
    pre_forward_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
    print(f"Forwardå‰å‚æ•°æ•°é‡: {pre_forward_params:,}")
    
    # æ‰§è¡Œforward
    print("æµ‹è¯•å‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids)
    
    print(f"âœ… MethodDbaseå‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"   è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
    
    # Forwardåå‚æ•°æ•°é‡
    post_forward_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
    print(f"Forwardåå‚æ•°æ•°é‡: {post_forward_params:,}")
    
    if post_forward_params != pre_forward_params:
        change = post_forward_params - pre_forward_params
        print(f"âœ… åŠ¨æ€å‚æ•°åˆ›å»º: +{change:,} å‚æ•°")
    else:
        print("âœ… æ— å‚æ•°å˜åŒ–ï¼ˆMLPå‚æ•°åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºï¼‰")


def compare_models():
    print("\n" + "="*70)
    print("æ¨¡å‹å¯¹æ¯”æ€»ç»“")
    print("="*70)
    
    config = MethodCbaseConfig_v4(
        vocab_size=1000,
        hidden_size=64,
        intermediate_size=128,
        num_hidden_layers=2,
        num_attention_heads=4,
        num_key_value_heads=4,
        max_position_embeddings=512,
        rms_norm_eps=1e-6,
        use_cache=True,
    )
    
    # åˆ›å»ºä¸¤ä¸ªæ¨¡å‹
    model_cbase = MethodCbaseLlamaForCausalLM_v4(config)
    model_dbase = MethodDbaseLlamaForCausalLM_v4(config)
    
    params_cbase_init = sum({p.data_ptr(): p.numel() for p in model_cbase.parameters()}.values())
    params_dbase_init = sum({p.data_ptr(): p.numel() for p in model_dbase.parameters()}.values())
    
    print(f"MethodCbaseåˆå§‹å‚æ•°: {params_cbase_init:,}")
    print(f"MethodDbaseåˆå§‹å‚æ•°: {params_dbase_init:,}")
    print(f"å‚æ•°å·®å¼‚: {params_dbase_init - params_cbase_init:+,}")
    
    # æ‰§è¡Œforwardæµ‹è¯•å‚æ•°å˜åŒ–
    input_ids = torch.randint(0, config.vocab_size, (1, 8))
    
    with torch.no_grad():
        model_cbase.eval()
        model_dbase.eval()
        
        _ = model_cbase(input_ids)
        _ = model_dbase(input_ids)
    
    params_cbase_after = sum({p.data_ptr(): p.numel() for p in model_cbase.parameters()}.values())
    params_dbase_after = sum({p.data_ptr(): p.numel() for p in model_dbase.parameters()}.values())
    
    print(f"\nForwardå:")
    print(f"MethodCbase: {params_cbase_after:,} (å˜åŒ–: {params_cbase_after - params_cbase_init:+,})")
    print(f"MethodDbase: {params_dbase_after:,} (å˜åŒ–: {params_dbase_after - params_dbase_init:+,})")
    
    print(f"\næ ¸å¿ƒå·®å¼‚:")
    print(f"- MethodCbase: æƒé‡åˆ—å‘é‡åœ¨ç¬¬ä¸€æ¬¡forwardæ—¶åŠ¨æ€åˆ›å»º")
    print(f"- MethodDbase: MLPå‚æ•°åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶åˆ›å»ºï¼Œæƒé‡åˆ—å‘é‡åŠ¨æ€è®¡ç®—")
    print(f"- ä¸¤è€…éƒ½åŸºäºåŸå§‹LlamaModelï¼Œåªå¯¹å½“å‰å±‚QK^Tåº”ç”¨æƒé‡")


if __name__ == "__main__":
    test_methodcbase()
    test_methoddbase()
    compare_models()
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼MethodCbaseå’ŒMethodDbaseæ¨¡å‹å®ç°æˆåŠŸ")

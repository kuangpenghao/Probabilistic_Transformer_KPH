#!/usr/bin/env python3
"""
æµ‹è¯•Method1Dçš„åŠ¨æ€æƒé‡ç”Ÿæˆæœºåˆ¶
"""

import torch
import sys
sys.path.append('.')

from models.version4.Method1D_v4 import Method1DLlamaForCausalLM_v4
from models.version4.configuration_llama_v4 import Method1DConfig_v4

def test_method1d_dynamic_weights():
    """æµ‹è¯•Method1Dçš„åŠ¨æ€æƒé‡ç”Ÿæˆæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("æµ‹è¯•Method1DåŠ¨æ€æƒé‡ç”Ÿæˆæœºåˆ¶")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = Method1DConfig_v4()
    print(f"é…ç½®: hidden_size={config.hidden_size}, num_layers={config.num_hidden_layers}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºMethod1Dæ¨¡å‹...")
    model = Method1DLlamaForCausalLM_v4(config)
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 16  # ä½¿ç”¨è¾ƒå°çš„åºåˆ—é•¿åº¦è¿›è¡Œæµ‹è¯•
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
    try:
        with torch.no_grad():
            output = model(input_ids)
            
        loss = output.loss if hasattr(output, 'loss') and output.loss is not None else "N/A"
        logits_shape = output.logits.shape if hasattr(output, 'logits') else "N/A"
        
        print(f"âœ… Method1Då‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   æŸå¤±: {loss}")
        print(f"   è¾“å‡ºlogitså½¢çŠ¶: {logits_shape}")
        
    except Exception as e:
        print(f"âŒ Method1Då‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    
    # è¯¦ç»†åˆ†æMLPå‚æ•°
    print("\n" + "=" * 80)
    print("MLPå‚æ•°åˆ†æ")
    print("=" * 80)
    
    def count_mlp_parameters(model):
        """ç»Ÿè®¡MLPç›¸å…³çš„å‚æ•°"""
        total_params = 0
        mlp_details = []
        
        for name, param in model.named_parameters():
            if "modified_scaling" in name and ("layer_mlps" in name or "layer_biases" in name or "layer_norms" in name):
                param_count = param.numel()
                total_params += param_count
                mlp_details.append(f"  {name}: {list(param.shape)} -> {param_count} å‚æ•°")
        
        return total_params, mlp_details
    
    total_mlp_params, mlp_details = count_mlp_parameters(model)
    print(f"MLPç›¸å…³å‚æ•°æ€»æ•°: {total_mlp_params}")
    
    # æŒ‰å±‚åˆ†ç»„æ˜¾ç¤º
    for i in range(config.num_hidden_layers):
        print(f"\nç¬¬{i}å±‚MLPç»„ä»¶:")
        layer_details = [detail for detail in mlp_details if f"modified_scaling.layer_" in detail and f".{i}." in detail]
        for detail in layer_details[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(detail)
        if len(layer_details) > 3:
            print(f"    ... è¿˜æœ‰ {len(layer_details) - 3} ä¸ªå‚æ•°")
    
    # è®¡ç®—ç†è®ºå‚æ•°é‡
    print(f"\nç†è®ºMLPå‚æ•°è®¡ç®—:")
    total_theoretical = 0
    for layer_idx in range(config.num_hidden_layers):
        # RMSNorm: hidden_size
        norm_params = config.hidden_size
        # Linear1: hidden_size * 4*hidden_size  
        linear1_params = config.hidden_size * 4 * config.hidden_size
        # Linear2: 4*hidden_size * (layer_idx + 1)
        linear2_params = 4 * config.hidden_size * (layer_idx + 1)
        # Bias: (layer_idx + 1)
        bias_params = layer_idx + 1
        
        layer_total = norm_params + linear1_params + linear2_params + bias_params
        total_theoretical += layer_total
        
        print(f"  ç¬¬{layer_idx}å±‚: {layer_total} å‚æ•° (è¾“å‡ºç»´åº¦: {layer_idx + 1})")
    
    print(f"ç†è®ºæ€»è®¡: {total_theoretical} å‚æ•°")
    print(f"å®é™…ç»Ÿè®¡: {total_mlp_params} å‚æ•°")
    print(f"åŒ¹é…åº¦: {'âœ…' if total_theoretical == total_mlp_params else 'âŒ'}")
    
    print("\nâœ… Method1Dæµ‹è¯•å®Œæˆï¼")
    return True

def test_dynamic_weight_matrix_generation():
    """æµ‹è¯•åŠ¨æ€æƒé‡çŸ©é˜µç”Ÿæˆçš„è¯¦ç»†è¿‡ç¨‹"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•åŠ¨æ€æƒé‡çŸ©é˜µç”Ÿæˆ")
    print("=" * 80)
    
    from models.version4.Method1D_v4 import v4m1D_ModifiedScailingComputation
    
    hidden_size = 64
    head_dim = 8
    num_layers = 2
    batch_size = 1
    seq_len = 8
    
    # åˆ›å»ºç¼©æ”¾è®¡ç®—æ¨¡å—
    scaling_module = v4m1D_ModifiedScailingComputation(hidden_size, head_dim, num_layers)
    
    # æ¨¡æ‹Ÿè¾“å…¥
    input_embedding = torch.randn(batch_size, seq_len, hidden_size)
    qk_matrices = [torch.randn(batch_size, 4, seq_len, seq_len) for _ in range(2)]  # å‡è®¾2ä¸ªQKçŸ©é˜µ
    
    print(f"è¾“å…¥åµŒå…¥å½¢çŠ¶: {input_embedding.shape}")
    print(f"QKçŸ©é˜µæ•°é‡: {len(qk_matrices)}")
    print(f"æ¯ä¸ªQKçŸ©é˜µå½¢çŠ¶: {qk_matrices[0].shape}")
    
    # æµ‹è¯•ç¬¬1å±‚ï¼ˆlayer_idx=1ï¼Œæœ‰2ä¸ªQKçŸ©é˜µï¼‰
    layer_idx = 1
    print(f"\næµ‹è¯•ç¬¬{layer_idx}å±‚ï¼ˆé¢„æœŸè¾“å‡ºç»´åº¦: {layer_idx + 1}ï¼‰")
    
    try:
        result = scaling_module.compute_modified_scaling(qk_matrices, layer_idx, input_embedding)
        print(f"âœ… åŠ¨æ€æƒé‡ç”ŸæˆæˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {result.shape}")
        print(f"   é¢„æœŸå½¢çŠ¶: {qk_matrices[0].shape}")
        print(f"   å½¢çŠ¶åŒ¹é…: {'âœ…' if result.shape == qk_matrices[0].shape else 'âŒ'}")
        
        # æ£€æŸ¥ä¸­é—´çš„æƒé‡çŸ©é˜µA_iç”Ÿæˆ
        print(f"\næƒé‡çŸ©é˜µA_iç”Ÿæˆè¿‡ç¨‹:")
        normed_input = scaling_module.layer_norms[layer_idx](input_embedding)
        mlp_output = scaling_module.layer_mlps[layer_idx](normed_input)
        weight_matrix_A = mlp_output + scaling_module.layer_biases[layer_idx]
        
        print(f"   normed_inputå½¢çŠ¶: {normed_input.shape}")
        print(f"   mlp_outputå½¢çŠ¶: {mlp_output.shape}")
        print(f"   weight_matrix_Aå½¢çŠ¶: {weight_matrix_A.shape}")
        print(f"   é¢„æœŸA_iå½¢çŠ¶: [{batch_size}, {seq_len}, {layer_idx + 1}]")
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€æƒé‡ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\nâœ… åŠ¨æ€æƒé‡çŸ©é˜µç”Ÿæˆæµ‹è¯•å®Œæˆï¼")
    return True

if __name__ == "__main__":
    success1 = test_method1d_dynamic_weights()
    success2 = test_dynamic_weight_matrix_generation()
    
    if success1 and success2:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Method1Då®ç°æˆåŠŸ")
    else:
        print(f"\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")

#!/usr/bin/env python3
"""
è¯¦ç»†æµ‹è¯•è„šæœ¬ï¼šä¸“é—¨éªŒè¯Method1_v4ä¸­ç¼©æ”¾ä¿®æ”¹çš„æ•°å­¦é€»è¾‘

éªŒè¯è¦ç‚¹ï¼š
1. éªŒè¯QK^TçŸ©é˜µçš„è®¡ç®—å’Œå­˜å‚¨æ˜¯å¦æ­£ç¡®
2. éªŒè¯scalingå¹¿æ’­æˆå‘é‡çš„é€»è¾‘
3. éªŒè¯å‘é‡ç‚¹ä¹˜çš„å®ç°
4. å¯¹æ¯”åŸå§‹scalingå’Œä¿®æ”¹åscalingçš„å·®å¼‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

import torch
import torch.nn as nn
import math
import numpy as np
from typing import List

from models.version4.Method1_v4 import (
    v4m1_ModifiedScailingComputation,
    Method1LlamaAttention_v4
)
from models.version4.configuration_llama_v4 import Method1Config_v4


class DetailedScalingTest:
    def __init__(self):
        """åˆå§‹åŒ–è¯¦ç»†æµ‹è¯•ç¯å¢ƒ"""
        self.device = torch.device("cpu")  # ä½¿ç”¨CPUä¾¿äºç²¾ç¡®æ•°å€¼éªŒè¯
        
        # ä½¿ç”¨ç®€å•çš„é…ç½®ä¾¿äºæ‰‹å·¥éªŒè¯
        self.config = Method1Config_v4(
            vocab_size=100,
            hidden_size=64,  # 64ç»´åº¦ï¼Œæ¯ä¸ªå¤´16ç»´
            num_hidden_layers=3,
            num_attention_heads=4,  # 4ä¸ªå¤´
            num_key_value_heads=4,
            max_position_embeddings=128,
        )
        
        self.head_dim = self.config.hidden_size // self.config.num_attention_heads
        print(f"å¤´ç»´åº¦: {self.head_dim}")
        print(f"åŸå§‹scaling: {1.0 / math.sqrt(self.head_dim)}")
    
    def test_scaling_computation_mathematics(self):
        """æµ‹è¯•ç¼©æ”¾è®¡ç®—çš„æ•°å­¦æ­£ç¡®æ€§"""
        print("\n=== æµ‹è¯•ç¼©æ”¾è®¡ç®—æ•°å­¦æ­£ç¡®æ€§ ===")
        
        scaling_comp = v4m1_ModifiedScailingComputation(self.head_dim)
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        batch_size, num_heads, seq_len = 1, 2, 4
        qk_shape = (batch_size, num_heads, seq_len, seq_len)
        
        # åˆ›å»ºå¯éªŒè¯çš„QK^TçŸ©é˜µ
        qk1 = torch.ones(qk_shape) * 2.0  # ç¬¬ä¸€å±‚ï¼šå…¨éƒ¨ä¸º2
        qk2 = torch.ones(qk_shape) * 3.0  # ç¬¬äºŒå±‚ï¼šå…¨éƒ¨ä¸º3
        qk3 = torch.ones(qk_shape) * 4.0  # ç¬¬ä¸‰å±‚ï¼šå…¨éƒ¨ä¸º4
        
        original_scaling = scaling_comp.original_scaling
        print(f"åŸå§‹scalingå€¼: {original_scaling}")
        
        # æµ‹è¯•ç¬¬0å±‚
        result_0 = scaling_comp.compute_modified_scaling([qk1], layer_idx=0)
        expected_0 = qk1 * original_scaling
        assert torch.allclose(result_0, expected_0), "ç¬¬0å±‚è®¡ç®—é”™è¯¯"
        print(f"âœ“ ç¬¬0å±‚: {qk1[0,0,0,0].item()} * {original_scaling} = {result_0[0,0,0,0].item()}")
        
        # æµ‹è¯•ç¬¬1å±‚
        result_1 = scaling_comp.compute_modified_scaling([qk1, qk2], layer_idx=1)
        expected_1 = original_scaling * (qk1 + qk2)
        assert torch.allclose(result_1, expected_1), "ç¬¬1å±‚è®¡ç®—é”™è¯¯"
        print(f"âœ“ ç¬¬1å±‚: {original_scaling} * ({qk1[0,0,0,0].item()} + {qk2[0,0,0,0].item()}) = {result_1[0,0,0,0].item()}")
        
        # æµ‹è¯•ç¬¬2å±‚
        result_2 = scaling_comp.compute_modified_scaling([qk1, qk2, qk3], layer_idx=2)
        expected_2 = original_scaling * (qk1 + qk2 + qk3)
        assert torch.allclose(result_2, expected_2), "ç¬¬2å±‚è®¡ç®—é”™è¯¯"
        print(f"âœ“ ç¬¬2å±‚: {original_scaling} * ({qk1[0,0,0,0].item()} + {qk2[0,0,0,0].item()} + {qk3[0,0,0,0].item()}) = {result_2[0,0,0,0].item()}")
        
        print("âœ“ ç¼©æ”¾è®¡ç®—æ•°å­¦æ­£ç¡®æ€§éªŒè¯é€šè¿‡")
    
    def test_vector_broadcasting_logic(self):
        """æµ‹è¯•å‘é‡å¹¿æ’­é€»è¾‘"""
        print("\n=== æµ‹è¯•å‘é‡å¹¿æ’­é€»è¾‘ ===")
        
        scaling_comp = v4m1_ModifiedScailingComputation(self.head_dim)
        
        # åˆ›å»ºä¸åŒæ•°é‡çš„QK^TçŸ©é˜µæ¥æµ‹è¯•å¹¿æ’­
        batch_size, num_heads, seq_len = 1, 1, 2
        qk_shape = (batch_size, num_heads, seq_len, seq_len)
        
        for num_matrices in [1, 2, 3, 5]:
            qk_matrices = [torch.randn(qk_shape) for _ in range(num_matrices)]
            
            if num_matrices == 1:
                # ç¬¬ä¸€å±‚ä¸ä½¿ç”¨å¹¿æ’­
                result = scaling_comp.compute_modified_scaling(qk_matrices, layer_idx=0)
                expected = qk_matrices[0] * scaling_comp.original_scaling
            else:
                # åç»­å±‚ä½¿ç”¨å¹¿æ’­
                result = scaling_comp.compute_modified_scaling(qk_matrices, layer_idx=num_matrices-1)
                expected = scaling_comp.original_scaling * sum(qk_matrices)
            
            assert torch.allclose(result, expected, atol=1e-6), f"{num_matrices}ä¸ªçŸ©é˜µçš„å¹¿æ’­è®¡ç®—é”™è¯¯"
            print(f"âœ“ {num_matrices}ä¸ªçŸ©é˜µçš„å‘é‡å¹¿æ’­æ­£ç¡®")
        
        print("âœ“ å‘é‡å¹¿æ’­é€»è¾‘éªŒè¯é€šè¿‡")
    
    def test_attention_integration(self):
        """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶çš„é›†æˆ"""
        print("\n=== æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶é›†æˆ ===")
        
        attention = Method1LlamaAttention_v4(self.config, layer_idx=1).to(self.device)
        attention.eval()
        
        batch_size, seq_len = 1, 4
        hidden_states = torch.randn(batch_size, seq_len, self.config.hidden_size).to(self.device)
        position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)
        
        # åˆ›å»ºä¹‹å‰å±‚çš„QK^TçŸ©é˜µ
        qk_shape = (batch_size, self.config.num_attention_heads, seq_len, seq_len)
        previous_qk = torch.randn(qk_shape).to(self.device)
        
        with torch.no_grad():
            # æ‰‹åŠ¨è®¡ç®—æœŸæœ›çš„ç»“æœ
            query_states = attention.q_proj(hidden_states)
            key_states = attention.k_proj(hidden_states)
            value_states = attention.v_proj(hidden_states)
            
            query_states = query_states.view(batch_size, seq_len, attention.num_heads, attention.head_dim).transpose(1, 2)
            key_states = key_states.view(batch_size, seq_len, attention.num_key_value_heads, attention.head_dim).transpose(1, 2)
            value_states = value_states.view(batch_size, seq_len, attention.num_key_value_heads, attention.head_dim).transpose(1, 2)
            
            # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
            cos, sin = attention.rotary_emb(value_states, position_ids)
            from transformers.models.llama.modeling_llama import apply_rotary_pos_emb
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            
            # è®¡ç®—å½“å‰å±‚çš„QK^T
            current_qk_expected = torch.matmul(query_states, key_states.transpose(2, 3))
            
            # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
            result = attention(
                hidden_states=hidden_states,
                position_ids=position_ids,
                previous_qk_matrices=[previous_qk],
                output_attentions=True
            )
            
            attn_output, attn_weights, past_key_value, current_qk_actual = result
            
            # éªŒè¯QK^TçŸ©é˜µçš„è®¡ç®—
            assert torch.allclose(current_qk_actual, current_qk_expected, atol=1e-5), \
                "QK^TçŸ©é˜µè®¡ç®—ä¸æ­£ç¡®"
            
            print(f"âœ“ QK^TçŸ©é˜µè®¡ç®—æ­£ç¡®ï¼Œå½¢çŠ¶: {current_qk_actual.shape}")
            
            # éªŒè¯ä¿®æ”¹åçš„ç¼©æ”¾æ˜¯å¦è¢«åº”ç”¨
            original_scaling = 1.0 / math.sqrt(attention.head_dim)
            expected_weighted = original_scaling * (previous_qk + current_qk_actual)
            
            # éªŒè¯softmaxä¹‹å‰çš„æƒé‡ï¼ˆæ³¨æ„causal maskçš„å½±å“ï¼‰
            print("âœ“ æ³¨æ„åŠ›æœºåˆ¶é›†æˆæ­£ç¡®")
    
    def test_end_to_end_scaling_effect(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯çš„ç¼©æ”¾æ•ˆæœ"""
        print("\n=== æµ‹è¯•ç«¯åˆ°ç«¯ç¼©æ”¾æ•ˆæœ ===")
        
        from models.version4.Method1_v4 import Method1LlamaModel_v4
        
        # åˆ›å»ºç®€å•é…ç½®
        simple_config = Method1Config_v4(
            vocab_size=50,
            hidden_size=32,
            num_hidden_layers=2,
            num_attention_heads=2,
            num_key_value_heads=2,
            max_position_embeddings=16,
        )
        
        model = Method1LlamaModel_v4(simple_config).to(self.device)
        model.eval()
        
        batch_size, seq_len = 1, 4
        input_ids = torch.randint(0, simple_config.vocab_size, (batch_size, seq_len)).to(self.device)
        
        with torch.no_grad():
            # è®°å½•æ¯å±‚çš„QK^TçŸ©é˜µç´¯ç§¯è¿‡ç¨‹
            inputs_embeds = model.embed_tokens(input_ids)
            position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)
            position_embeddings = model.rotary_emb(inputs_embeds, position_ids)
            
            hidden_states = inputs_embeds
            stored_qk_matrices = []
            layer_scaling_effects = []
            
            for layer_idx, decoder_layer in enumerate(model.layers):
                # è®°å½•ä½¿ç”¨ä¿®æ”¹ç¼©æ”¾å‰çš„QK^T
                attention_layer = decoder_layer.self_attn
                
                # æ‰‹åŠ¨è®¡ç®—QK^Tä»¥éªŒè¯ç´¯ç§¯æ•ˆæœ
                normalized_hidden = decoder_layer.input_layernorm(hidden_states)
                
                query_states = attention_layer.q_proj(normalized_hidden)
                key_states = attention_layer.k_proj(normalized_hidden)
                
                query_states = query_states.view(batch_size, seq_len, attention_layer.num_heads, attention_layer.head_dim).transpose(1, 2)
                key_states = key_states.view(batch_size, seq_len, attention_layer.num_key_value_heads, attention_layer.head_dim).transpose(1, 2)
                
                cos, sin = position_embeddings
                from transformers.models.llama.modeling_llama import apply_rotary_pos_emb
                query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
                
                current_qk = torch.matmul(query_states, key_states.transpose(2, 3))
                
                # è®¡ç®—ä¿®æ”¹åçš„ç¼©æ”¾æ•ˆæœ
                all_qk = stored_qk_matrices + [current_qk]
                modified_scaling_effect = attention_layer.modified_scaling.compute_modified_scaling(all_qk, layer_idx)
                
                layer_scaling_effects.append({
                    'layer_idx': layer_idx,
                    'num_qk_matrices': len(all_qk),
                    'current_qk_mean': current_qk.mean().item(),
                    'modified_scaling_mean': modified_scaling_effect.mean().item(),
                })
                
                # æ‰§è¡Œå®Œæ•´çš„å±‚å‰å‘ä¼ æ’­
                layer_outputs = decoder_layer(
                    hidden_states=hidden_states,
                    position_embeddings=position_embeddings,
                    previous_qk_matrices=stored_qk_matrices,
                    output_attentions=True
                )
                
                hidden_states = layer_outputs[0]
                stored_qk_matrices.append(layer_outputs[-1])
                
                print(f"ç¬¬{layer_idx}å±‚: ä½¿ç”¨äº†{len(all_qk)}ä¸ªQK^TçŸ©é˜µ, ä¿®æ”¹åç¼©æ”¾å‡å€¼: {modified_scaling_effect.mean().item():.6f}")
            
            print("âœ“ ç«¯åˆ°ç«¯ç¼©æ”¾æ•ˆæœéªŒè¯é€šè¿‡")
            
            # æ‰“å°ç¼©æ”¾æ•ˆæœæ€»ç»“
            print("\nç¼©æ”¾æ•ˆæœæ€»ç»“:")
            for effect in layer_scaling_effects:
                print(f"  å±‚{effect['layer_idx']}: {effect['num_qk_matrices']}ä¸ªçŸ©é˜µ -> ä¿®æ”¹ç¼©æ”¾å‡å€¼: {effect['modified_scaling_mean']:.6f}")
    
    def test_mathematical_equivalence(self):
        """æµ‹è¯•æ•°å­¦ç­‰ä»·æ€§"""
        print("\n=== æµ‹è¯•æ•°å­¦ç­‰ä»·æ€§ ===")
        
        scaling_comp = v4m1_ModifiedScailingComputation(self.head_dim)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size, num_heads, seq_len = 2, 3, 5
        qk_shape = (batch_size, num_heads, seq_len, seq_len)
        
        # æµ‹è¯•å¤šç§æƒ…å†µ
        test_cases = [
            ([torch.randn(qk_shape)], 0),
            ([torch.randn(qk_shape), torch.randn(qk_shape)], 1),
            ([torch.randn(qk_shape) for _ in range(3)], 2),
            ([torch.randn(qk_shape) for _ in range(4)], 3),
        ]
        
        for qk_matrices, layer_idx in test_cases:
            result = scaling_comp.compute_modified_scaling(qk_matrices, layer_idx)
            
            if layer_idx == 0:
                # ç¬¬ä¸€å±‚åº”è¯¥ç­‰äºåŸå§‹ç¼©æ”¾
                expected = qk_matrices[0] * scaling_comp.original_scaling
            else:
                # åç»­å±‚åº”è¯¥ç­‰äºscaling * sum(æ‰€æœ‰QKçŸ©é˜µ)
                expected = scaling_comp.original_scaling * sum(qk_matrices)
            
            assert torch.allclose(result, expected, atol=1e-6), \
                f"å±‚{layer_idx}çš„æ•°å­¦ç­‰ä»·æ€§éªŒè¯å¤±è´¥"
            
            print(f"âœ“ å±‚{layer_idx}({len(qk_matrices)}ä¸ªçŸ©é˜µ): æ•°å­¦ç­‰ä»·æ€§éªŒè¯é€šè¿‡")
        
        print("âœ“ æ•°å­¦ç­‰ä»·æ€§éªŒè¯å®Œå…¨é€šè¿‡")
    
    def run_detailed_tests(self):
        """è¿è¡Œæ‰€æœ‰è¯¦ç»†æµ‹è¯•"""
        print("å¼€å§‹è¿è¡ŒMethod1_v4è¯¦ç»†ç¼©æ”¾é€»è¾‘æµ‹è¯•...")
        print("=" * 70)
        
        try:
            self.test_scaling_computation_mathematics()
            self.test_vector_broadcasting_logic()
            self.test_attention_integration()
            self.test_end_to_end_scaling_effect()
            self.test_mathematical_equivalence()
            
            print("\n" + "=" * 70)
            print("ğŸ‰ æ‰€æœ‰è¯¦ç»†æµ‹è¯•é€šè¿‡ï¼Method1_v4ç¼©æ”¾é€»è¾‘æ­£ç¡®å®ç°")
            print("=" * 70)
            
            print("\nè®¾è®¡æ–¹æ¡ˆè¯¦ç»†éªŒè¯:")
            print("âœ“ QK^TçŸ©é˜µæ­£ç¡®è®¡ç®—å’Œå­˜å‚¨")
            print("âœ“ åŸå§‹sqrt(d_k)æ­£ç¡®å¹¿æ’­æˆå‘é‡")
            print("âœ“ å‘é‡ä¸QK^TçŸ©é˜µç»„åˆæ­£ç¡®è¿›è¡Œç‚¹ä¹˜")
            print("âœ“ ä¿®æ”¹åçš„ç¼©æ”¾åœ¨æ¯å±‚ä¸­æ­£ç¡®ç´¯ç§¯åº”ç”¨")
            print("âœ“ æ•°å­¦è®¡ç®—ä¸è®¾è®¡æ–¹æ¡ˆå®Œå…¨ä¸€è‡´")
            
        except Exception as e:
            print(f"\nâŒ è¯¦ç»†æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        return True


if __name__ == "__main__":
    tester = DetailedScalingTest()
    success = tester.run_detailed_tests()
    
    if success:
        print("\nâœ… è¯¦ç»†æµ‹è¯•å®Œæˆï¼šMethod1_v4ç¼©æ”¾é€»è¾‘å®Œå…¨ç¬¦åˆè®¾è®¡æ–¹æ¡ˆ")
    else:
        print("\nâŒ è¯¦ç»†æµ‹è¯•å¤±è´¥ï¼šè¯·æ£€æŸ¥ç¼©æ”¾é€»è¾‘å®ç°")
        sys.exit(1)

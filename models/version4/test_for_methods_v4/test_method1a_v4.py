#!/usr/bin/env python3
"""
Method1A_v4æ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æˆåŠŸè®­ç»ƒï¼Œå¹¶éªŒè¯å¯å­¦ä¹ æƒé‡çŸ©é˜µå‚æ•°æ˜¯å¦æ­£ç¡®æ›´æ–°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import json
import os
import sys

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥æ¨¡å‹
sys.path.append('/home/kuangph/hf-starter')

from models.version4.Method1A_v4 import Method1ALlamaForCausalLM_v4, Method1AConfig_v4
from transformers import AutoTokenizer


def create_test_config():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„æ¨¡å‹é…ç½®"""
    config = Method1AConfig_v4(
        vocab_size=1000,
        hidden_size=128,
        intermediate_size=256,
        num_hidden_layers=4,
        num_attention_heads=4,
        num_key_value_heads=2,
        max_position_embeddings=256,
        model_type="method1a-v4",
        torch_dtype="float32"
    )
    return config


def create_dummy_dataset(vocab_size=1000, seq_len=64, num_samples=100):
    """åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†ç”¨äºæµ‹è¯•"""
    # ç”Ÿæˆéšæœºè¾“å…¥åºåˆ—
    input_ids = torch.randint(0, vocab_size, (num_samples, seq_len))
    
    # æ ‡ç­¾æ˜¯è¾“å…¥å³ç§»ä¸€ä½
    labels = torch.cat([input_ids[:, 1:], torch.zeros(num_samples, 1, dtype=torch.long)], dim=1)
    
    return TensorDataset(input_ids, labels)


def get_weight_matrices_state(model):
    """è·å–æ‰€æœ‰æƒé‡çŸ©é˜µçš„å½“å‰çŠ¶æ€"""
    weight_states = {}
    
    for layer_idx, layer in enumerate(model.model.layers):
        scaling_module = layer.self_attn.modified_scaling
        if scaling_module.layer_initialized[layer_idx]:
            weight_states[layer_idx] = []
            num_weights = len(scaling_module.layer_weight_matrices[layer_idx])
            for i in range(num_weights):
                weight_matrix = scaling_module.layer_weight_matrices[layer_idx][i]
                weight_states[layer_idx].append(weight_matrix.data.clone())
    
    return weight_states


def compare_weight_states(state1, state2, tolerance=1e-6):
    """æ¯”è¾ƒä¸¤ä¸ªæƒé‡çŠ¶æ€ï¼Œè¿”å›æ˜¯å¦æœ‰å˜åŒ–"""
    changes = {}
    
    for layer_idx in state1:
        if layer_idx in state2:
            layer_changes = []
            for i, (w1, w2) in enumerate(zip(state1[layer_idx], state2[layer_idx])):
                diff = torch.abs(w1 - w2).max().item()
                layer_changes.append(diff)
            changes[layer_idx] = layer_changes
    
    return changes


def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("=" * 50)
    print("æµ‹è¯•1: æ¨¡å‹åˆå§‹åŒ–")
    print("=" * 50)
    
    try:
        config = create_test_config()
        model = Method1ALlamaForCausalLM_v4(config)
        print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ“ æ€»å‚æ•°é‡: {total_params:,}")
        print(f"âœ“ å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        
        return model, config
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return None, None


def test_forward_pass(model, config):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•2: å‰å‘ä¼ æ’­")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        seq_len = 32
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        labels = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        
        print(f"âœ“ è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(input_ids=input_ids, labels=labels)
            
        print(f"âœ“ è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        print(f"âœ“ æŸå¤±å€¼: {outputs.loss.item():.4f}")
        
        # æ£€æŸ¥æƒé‡çŸ©é˜µæ˜¯å¦è¢«æ­£ç¡®åˆå§‹åŒ–
        initialized_layers = []
        for layer_idx, layer in enumerate(model.model.layers):
            scaling_module = layer.self_attn.modified_scaling
            if scaling_module.layer_initialized[layer_idx]:
                num_weights = len(scaling_module.layer_weight_matrices[layer_idx])
                initialized_layers.append((layer_idx, num_weights))
                print(f"âœ“ ç¬¬{layer_idx}å±‚åˆå§‹åŒ–äº†{num_weights}ä¸ªæƒé‡çŸ©é˜µ")
        
        return True, initialized_layers
        
    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, []


def test_weight_learning(model, config):
    """æµ‹è¯•æƒé‡çŸ©é˜µæ˜¯å¦èƒ½è¢«å­¦ä¹ """
    print("\n" + "=" * 50)
    print("æµ‹è¯•3: æƒé‡çŸ©é˜µå­¦ä¹ èƒ½åŠ›")
    print("=" * 50)
    
    try:
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        dataset = create_dummy_dataset(config.vocab_size, seq_len=32, num_samples=20)
        dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        
        # è®°å½•åˆå§‹æƒé‡çŠ¶æ€
        print("è®°å½•åˆå§‹æƒé‡çŠ¶æ€...")
        initial_weights = get_weight_matrices_state(model)
        
        # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
        model.train()
        num_steps = 5
        losses = []
        
        print(f"å¼€å§‹è®­ç»ƒ {num_steps} æ­¥...")
        for step, (input_ids, labels) in enumerate(dataloader):
            if step >= num_steps:
                break
                
            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            losses.append(loss.item())
            print(f"æ­¥éª¤ {step+1}/{num_steps}, æŸå¤±: {loss.item():.4f}")
        
        # è®°å½•è®­ç»ƒåæƒé‡çŠ¶æ€
        print("è®°å½•è®­ç»ƒåæƒé‡çŠ¶æ€...")
        final_weights = get_weight_matrices_state(model)
        
        # æ¯”è¾ƒæƒé‡å˜åŒ–
        print("\næƒé‡çŸ©é˜µå˜åŒ–åˆ†æ:")
        changes = compare_weight_states(initial_weights, final_weights)
        
        total_changed_weights = 0
        for layer_idx, layer_changes in changes.items():
            print(f"ç¬¬{layer_idx}å±‚:")
            for i, change in enumerate(layer_changes):
                status = "âœ“ å·²æ›´æ–°" if change > 1e-6 else "âœ— æœªæ›´æ–°"
                print(f"  æƒé‡çŸ©é˜µ{i}: æœ€å¤§å˜åŒ– = {change:.2e} {status}")
                if change > 1e-6:
                    total_changed_weights += 1
        
        print(f"\næ€»ç»“: {total_changed_weights} ä¸ªæƒé‡çŸ©é˜µè¢«æˆåŠŸæ›´æ–°")
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸‹é™
        if len(losses) > 1:
            loss_improvement = losses[0] - losses[-1]
            print(f"æŸå¤±æ”¹å–„: {loss_improvement:.4f} ({'âœ“ ä¸‹é™' if loss_improvement > 0 else 'âœ— æœªä¸‹é™'})")
        
        return total_changed_weights > 0, changes
        
    except Exception as e:
        print(f"âœ— æƒé‡å­¦ä¹ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, {}


def test_weight_matrix_properties(model):
    """æµ‹è¯•æƒé‡çŸ©é˜µçš„å±æ€§"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•4: æƒé‡çŸ©é˜µå±æ€§éªŒè¯")
    print("=" * 50)
    
    try:
        # æ£€æŸ¥æ¯ä¸€å±‚çš„æƒé‡çŸ©é˜µæ•°é‡å’Œå½¢çŠ¶
        for layer_idx, layer in enumerate(model.model.layers):
            scaling_module = layer.self_attn.modified_scaling
            if scaling_module.layer_initialized[layer_idx]:
                expected_num_weights = layer_idx + 1
                actual_num_weights = len(scaling_module.layer_weight_matrices[layer_idx])
                
                print(f"ç¬¬{layer_idx}å±‚:")
                print(f"  æœŸæœ›æƒé‡çŸ©é˜µæ•°é‡: {expected_num_weights}")
                print(f"  å®é™…æƒé‡çŸ©é˜µæ•°é‡: {actual_num_weights}")
                
                if expected_num_weights == actual_num_weights:
                    print("  âœ“ æƒé‡çŸ©é˜µæ•°é‡æ­£ç¡®")
                else:
                    print("  âœ— æƒé‡çŸ©é˜µæ•°é‡é”™è¯¯")
                
                # æ£€æŸ¥æ¯ä¸ªæƒé‡çŸ©é˜µçš„å½¢çŠ¶å’Œå±æ€§
                for i in range(actual_num_weights):
                    weight_matrix = scaling_module.layer_weight_matrices[layer_idx][i]
                    print(f"  æƒé‡çŸ©é˜µ{i}: å½¢çŠ¶ {weight_matrix.shape}, requires_grad={weight_matrix.requires_grad}")
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºé›¶åˆå§‹åŒ–
                    is_zero_init = torch.allclose(weight_matrix, torch.zeros_like(weight_matrix))
                    print(f"    é›¶åˆå§‹åŒ–: {'âœ“' if is_zero_init else 'âœ—'}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æƒé‡çŸ©é˜µå±æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_gradient_flow(model, config):
    """æµ‹è¯•æ¢¯åº¦æµæ˜¯å¦æ­£å¸¸"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•5: æ¢¯åº¦æµæµ‹è¯•")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ - ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„åºåˆ—é•¿åº¦
        input_ids = torch.randint(0, config.vocab_size, (1, 32))
        labels = torch.randint(0, config.vocab_size, (1, 32))
        
        # å‰å‘ä¼ æ’­
        model.train()
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æƒé‡çŸ©é˜µçš„æ¢¯åº¦
        gradient_stats = {}
        for layer_idx, layer in enumerate(model.model.layers):
            scaling_module = layer.self_attn.modified_scaling
            if scaling_module.layer_initialized[layer_idx]:
                layer_grads = []
                num_weights = len(scaling_module.layer_weight_matrices[layer_idx])
                for i in range(num_weights):
                    weight_matrix = scaling_module.layer_weight_matrices[layer_idx][i]
                    if weight_matrix.grad is not None:
                        grad_norm = weight_matrix.grad.norm().item()
                        layer_grads.append(grad_norm)
                        print(f"ç¬¬{layer_idx}å±‚æƒé‡çŸ©é˜µ{i}: æ¢¯åº¦èŒƒæ•° = {grad_norm:.2e}")
                    else:
                        layer_grads.append(0.0)
                        print(f"ç¬¬{layer_idx}å±‚æƒé‡çŸ©é˜µ{i}: æ— æ¢¯åº¦")
                
                gradient_stats[layer_idx] = layer_grads
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
        has_gradients = any(any(grad > 0 for grad in layer_grads) 
                           for layer_grads in gradient_stats.values())
        
        if has_gradients:
            print("âœ“ æƒé‡çŸ©é˜µæ¢¯åº¦æµæ­£å¸¸")
        else:
            print("âœ— æƒé‡çŸ©é˜µæ²¡æœ‰æ¢¯åº¦")
        
        return has_gradients, gradient_stats
        
    except Exception as e:
        print(f"âœ— æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, {}


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("Method1A_v4 æ¨¡å‹æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    test_results = {}
    
    # æµ‹è¯•1: æ¨¡å‹åˆå§‹åŒ–
    model, config = test_model_initialization()
    if model is None:
        print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    test_results['initialization'] = True
    
    # æµ‹è¯•2: å‰å‘ä¼ æ’­
    forward_success, initialized_layers = test_forward_pass(model, config)
    test_results['forward_pass'] = forward_success
    
    if not forward_success:
        print("å‰å‘ä¼ æ’­å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # æµ‹è¯•3: æƒé‡çŸ©é˜µå±æ€§
    properties_success = test_weight_matrix_properties(model)
    test_results['weight_properties'] = properties_success

    # æµ‹è¯•4: æƒé‡çŸ©é˜µå­¦ä¹ 
    learning_success, weight_changes = test_weight_learning(model, config)
    test_results['weight_learning'] = learning_success    
    
    # æµ‹è¯•5: æ¢¯åº¦æµ
    gradient_success, gradient_stats = test_gradient_flow(model, config)
    test_results['gradient_flow'] = gradient_success
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    for test_name, success in test_results.items():
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
    
    all_passed = all(test_results.values())
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Method1A_v4æ¨¡å‹å¯ä»¥æ­£å¸¸è®­ç»ƒï¼Œæƒé‡çŸ©é˜µå‚æ•°èƒ½å¤Ÿè¢«æˆåŠŸå­¦ä¹ ã€‚")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®ç°ã€‚")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
